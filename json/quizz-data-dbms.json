{

	"Systems": "SECTION",
	"Systems - Distributed - Principles - Consistency": "In distributed systems, consistency models 𝗱𝗲𝗳𝗶𝗻𝗲 𝗵𝗼𝘄 𝗱𝗮𝘁𝗮 𝘂𝗽𝗱𝗮𝘁𝗲𝘀 𝗮𝗿𝗲 𝘀𝗲𝗲𝗻 𝗮𝗰𝗿𝗼𝘀𝘀 𝗻𝗼𝗱𝗲𝘀. Strong consistency and eventual consistency 𝗿𝗲𝗽𝗿𝗲𝘀𝗲𝗻𝘁 𝗯𝗼𝘁𝗵 𝗲𝗻𝗱𝘀 𝗼𝗳 𝘁𝗵𝗲 𝗰𝗼𝗻𝘀𝗶𝘀𝘁𝗲𝗻𝗰𝘆 𝘀𝗽𝗲𝗰𝘁𝗿𝘂𝗺 (sequential and causal consistency in between).",
	"Systems - Distributed - Principles - Consistency - Strong": "↳ Every read sees the latest write ↳ Higher latency, lower availability, higher integrity ↳ Ideal for real-time correctness",
	"Systems - Distributed - Principles - Consistency - Sequential": ".",
	"Systems - Distributed - Principles - Consistency - Causal": ".",
	"Systems - Distributed - Principles - Consistency - Weak": "It is a consistency model used in distributed computing where subsequent accesses might not always be returning the updated value. There might be inconsistent responses.",
	"Systems - Distributed - Principles - Consistency - Eventual": "↳ Reads may return stale data temporarily ↳ Lower latency, higher availability ↳ Great for scale-first systems .",
	"Systems - Distributed - Principles - HA - Scalability": "",
	"Systems - Distributed - Principles - HA - Scalability - Dimensions": "In their book “The Art of Scalability,” Martin Abbott and Michael Fisher elaborate on the concept of the “scale cube,”. (X) Each service can then be further scaled by cloning (X-axis), (Y) The microservices pattern maps to the Y-axis of the cube, wherein functional decomposition is used to scale the system (Z) sharding (Z-axis).",
	"Systems - Distributed - Principles - HA - Scalability - ScaleCube - X - clone services (horizontal duplication)": "X-axis scaling consists of running multiple copies of an application behind a load balancer. If there are N copies then each copy handles 1/N of the load. This is a simple, commonly used approach of scaling an application. One drawback of this approach is that because each copy potentially accesses all of the data, caches require more memory to be effective. Another problem with this approach is that it does not tackle the problems of increasing development and application complexity.",
	"Systems - Distributed - Principles - HA - Scalability - ScaleCube - y - microservices (functional decomposition)": "Unlike X-axis and Z-axis, which consist of running multiple, identical copies of the application, Y-axis axis scaling splits the application into multiple, different services. Each service is responsible for one or more closely related functions. There are a couple of different ways of decomposing the application into services. One approach is to use verb-based decomposition and define services that implement a single use case such as checkout. The other option is to decompose the application by noun and create services responsible for all operations related to a particular entity such as customer management. An application might use a combination of verb-based and noun-based decomposition.",
	"Systems - Distributed - Principles - HA - Scalability - ScaleCube - z - sharding databases (data partioning)": "When using Z-axis scaling each server runs an identical copy of the code. In this respect, it’s similar to X-axis scaling. The big difference is that each server is responsible for only a subset of the data. Some component of the system is responsible for routing each request to the appropriate server. One commonly used routing criteria is an attribute of the request such as the primary key of the entity being accessed. Another common routing criteria is the customer type. For example, an application might provide paying customers with a higher SLA than free customers by routing their requests to a different set of servers with more capacity.",
	"Systems - Distributed - Theorems - CAP Theorem ": "In any distributed data store, you can have at most two of these three properties: consistency (C), high availability (A), tolerance to network partitions (P)",
	"Systems - Distributed - Theorems - CAP Theorem - Consistency (C)": "Consistency (C) – Every read receives the most recent write or an error.",
	"Systems - Distributed - Theorems - CAP Theorem - Availability (A)": "Availability (A) – Every request (read/write) gets a response, even if it's not the most recent data.",
	"Systems - Distributed - Theorems - CAP Theorem - PartitionTolerance (P)": "Partition Tolerance (P) – The system continues operating despite network failures between nodes.",
	"Systems - Distributed - Theorems - CAP Theorem - Solutions - CA - Forfeit Partitions": "The system achieves High Available Consistency. To manage data across multiple resources these systems used methods like 2-Phase Commit. Single-site databases, RDBMS (Oracle, Postgres, MySQL) could be good eamples of CA systems.",
	"Systems - Distributed - Theorems - CAP Theorem - Solutions - CP - Best Effort Availity": "The system achieves Strong Consistency with Partition Tolerance. Pessimistic Locking methods used for multi resource data management. DNS, MongoDB, HBase, Redis are the example CP systems.",
	"Systems - Distributed - Theorems - CAP Theorem - Solutions - AP - Best Effort Consistency": "The system offers Full Availability by relaxing consistency. Optimistic Locking methods used for multi resource data management. CouchBase, Cassandra, DynamoDB, Hazelcast are the example CP systems.",
	"Systems - Decentralized - Theorems - DCS Theorem": "",
	"Systems - Decentralized - Theorems - DCS Theorem - Decentralization": "Decentralized means the system has no single point of failure or control (SPoF). Another way to state this is: if any single element is removed from {𝑆}, the system continues to perform its intended behavior, and no single component in {𝑆} has the power to redefine 𝑓𝑆 on its own.",
	"Systems - Decentralized - Theorems - DCS Theorem - Consensus": "Consensus means the system uses a collective decision-making process ('consensus algorithm') to update the system’s state, 𝑠, which is shared by all consensus participants. The result of the consensus algorithm determines the network’s accepted output of 𝑓𝑆 , and whether or not 𝑓𝑆 completes within 𝑆𝜏.",
	"Systems - Decentralized - Theorems - DCS Theorem - Scalability": "Scale means the system is capable of handling the transactional demands of any competing system providing the same service to the same arbitrary set of users across the globe (“at scale”).2",

	"Consuming": "SECTION",
	"Consuming - Caching - purpose": "concept of storing frequently used data in a temporary location that is faster to access than the original source.",
	"Consuming - Caching - Strategies - ReadOptimized - ReadAside": "◾ The application checks the cache first for data. ◾ If found (cache hit), it's returned. ◾ If not (cache miss), the data is fetched from the database, stored in the cache and then returned. ◾ Referred as 'lazy loading' => on-demand data fetching. ◾ The first request for a particular piece of data will always result in a cache miss because the data hasn't been loaded yet. ◾ It offers Simplicity. Use for ◾ Read-heavy workloads with infrequently changing data.",
	"Consuming - Caching - Strategies - ReadOptimized - ReadThrough": "◾ The cache acts as the primary data source. ◾ If data isn't in the cache : - it's retrieved from the database - stored in the cache - and then returned to the application. ◾ It is basically abstracting the data source from the application. Use for ◾ Applications where extremely fast reads are critical. (Real time analytics etc.).",
	"Consuming - Caching - Strategies - ReadOptimized - RefreshAhead": "◾ The cache proactively refreshes data before it expires, reducing the chance of cache misses. Use for ◾ Applications with predictable data access patterns.",
	"Consuming - Caching - Strategies - ReadOptimized - ClientSideCaching": "◾ Data is cached on the client's device. **Increased complexity, requires managing cache invalidation on the client-side. Use for Applications with frequently accessed static content or when personalization is not a major concern. ",
	"Consuming - Caching - Strategies - WriteOptimized - WriteThrough": "◾ Data is written first to the cache and then to the database 'immediately'. ◾ The write operation is considered complete only when both the cache and database have been updated. ◾ Ensures strong data consistency between the cache and database. When to use? ◾ Applications where data consistency is critical..",
	"Consuming - Caching - Strategies - WriteOptimized - WriteBack": "◾ Data is written to the cache first and then to the database 'asynchronously '. Use for ◾ Applications where write performance is a priority and some data loss is tolerable..",
	"Consuming - Caching - Strategies - WriteOptimized - WriteAround": "◾ Bypasses the cache for writes, writing directly to the database. ◾ The cache is updated only on subsequent reads. Use for ◾ Write-heavy workloads where most data is written once and rarely read..",
	"Consuming - Caching - Strategies - Eviction - LRU": "𝐋𝐞𝐚𝐬𝐭 𝐑𝐞𝐜𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐋𝐑𝐔) - Removes the least recently accessed item first. - Works well when older data is less likely to be used again. - Example: Browsers use LRU to discard old pages from memory when opening new tabs.",
	"Consuming - Caching - Strategies - Eviction - LFU": "𝐋𝐞𝐚𝐬𝐭 𝐅𝐫𝐞𝐪𝐮𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐋𝐅𝐔) - Evicts the least accessed items over time. - Prioritizes keeping frequently used items in cache. - Example: CDNs use LFU to keep trending videos in cache while removing rarely watched ones.",
	"Consuming - Caching - Strategies - Eviction - MRU": "𝐌𝐨𝐬𝐭 𝐑𝐞𝐜𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐌𝐑𝐔) - Opposite of LRU – evicts the most recently accessed item first. - Useful when recent data becomes obsolete quickly. - Example: In a music streaming app, the last played song is less likely to be played again, making MRU a better choice.",
	"Consuming - Caching - Strategies - Eviction - TTL": "𝐓𝐢𝐦𝐞 𝐭𝐨 𝐋𝐢𝐯𝐞 (𝐓𝐓𝐋) - Items are evicted after a set time limit (expiry time). - Prevents stale data, useful in distributed systems. - Example: DNS records have TTL values, so if an IP address changes, clients don’t hold onto outdated mappings.",
	"Consuming - Caching - Strategies - Eviction - FIFO": "𝐅𝐢𝐫𝐬𝐭 𝐈𝐧, 𝐅𝐢𝐫𝐬𝐭 𝐎𝐮𝐭 (𝐅𝐈𝐅𝐎) - Evicts the oldest stored item first, regardless of usage. - Simple to implement but may remove still-relevant data. - Example: Simple queue-based caching systems.",
	"Consuming - Caching - Strategies - Eviction - RR": "𝐑𝐚𝐧𝐝𝐨𝐦 𝐑𝐞𝐩𝐥𝐚𝐜𝐞𝐦𝐞𝐧𝐭 (𝐑𝐑) - Randomly evicts an item when the cache is full. - Low overhead, but less predictable performance. - Example: Some network routers use RR for managing limited memory, avoiding complex eviction policies.",
	"Consuming - Caching - Strategies - Eviction - TT": "𝐓𝐰𝐨-𝐓𝐢𝐞𝐫𝐞𝐝 𝐂𝐚𝐜𝐡𝐢𝐧𝐠 - Uses a fast in-memory cache (e.g., Redis) & a slower persistent cache (e.g., disk-based). - Optimizes speed & storage by balancing hot and cold data. - Example: A CDN stores frequently accessed content in RAM (L1 cache) while keeping less-accessed content on disk (L2 cache) to balance performance and cost.",
	"Consuming - Pagination": "",
	"Consuming - Pagination - OffsetBased": "This technique uses an offset and a limit parameter to define the starting point and the number of records to return. Example: GET /orders?offset=0&limit=3 ",
	"Consuming - Pagination - CursorBased": "This technique uses a cursor (a unique identifier) to mark the position in the dataset. Typically, the cursor is an encoded string that points to a specific record. Example: GET /orders?cursor=xxx ",
	"Consuming - Pagination - PageBased": "This technique specifies the page number and the size of each page. Example: GET /items?page=2&size=3 ",
	"Consuming - Pagination - KeysetBased": "This technique uses a key to filter the dataset, often the primary key or another indexed column. Example: GET /items?after_id=102&limit=3 ",
	"Consuming - Pagination - TimeBased": "This technique uses a timestamp or date to paginate through records. Example: GET /items?start_time=xxx&end_time=yyy ",
	"Consuming - Pagination - Hybrid": "This technique combines multiple pagination techniques to leverage their strengths. Example: Combining cursor and time-based pagination for efficient scrolling through time-ordered records. ",

	"Querying": "SECTION",
	"Querying - Transactions": "",
	"Querying - Transactions - Principles - Data - ACID": "ACID: Atomicity, Consistency, Isolation, Durability",
	"Querying - Transactions - Principles - Data - ACID - Atomicity": "The DB must treat each transaction as all or nothing, if any part fails, the whole transaction must be rolled back as if it never happened.",
	"Querying - Transactions - Principles - Data - ACID - Atomicity - Orthogonality": "Atomicity does not behave completely orthogonally with regard to the other ACID properties of transactions. For example, isolation relies on atomicity to roll back the enclosing transaction in the event of an isolation violation such as a deadlock; consistency also relies on atomicity to roll back the enclosing transaction in the event of a consistency violation by an illegal transaction. As a result of this, a failure to detect a violation and roll back the enclosing transaction may cause an isolation or consistency failure.",
	"Querying - Transactions - Principles - Data - ACID - Consistency": "A catch-all term that means all the rules defined in the database must be followed when committing a transaction. The end state, at the end of the transaction, must be valid. Rules here can mean data constraints, cascades, triggers, etc. Consistency relies on atomicity, in that if there is a violation, we rely on the systems ability to roll back changes.",
	"Querying - Transactions - Principles - Data - ACID - Isolation": "The isolation determines the visibility of changes made by one transaction for another transactions. Ideally, the result of 2 concurrent operations should be the same as if they occurred sequentially.",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions": "A transaction is an unit of work that helps to respect ACID principle. This unit of work can only be executed entirely. It means that database will see zero (if transaction fails, operation called rollback) or all operations placed inside transaction (if transaction succeeds, operation called commit).",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions - levels": "They're 4 mains transaction's isolation levels: read uncommited, read commited, repeatable read, serializable",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions - levels - ReadUncommited": "imagine two transactions, 'A' and 'B'. First, 'A' writes a data into one table without commiting the transaction. After, 'B' reads the uncommited data and work on it. But some error occurs on commiting the 'A' transaction and all changes are rollbacked. In this case, 'B' continues to work on uncommited data by the 'A' transaction. This mode is very fast but can introduce a lot of data consistency problems",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions - levels - ReadCommitted": "we still use the same scenario as for read uncommited, but commited data is locked. It means that 'B' can't see uncommited data from the 'A' transaction. 'B' can see it only when 'A' will commit its transaction.",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions - levels - RepeatableRead": "this isolation level promotes the same data read, even if the data was changed meanwhile. We continue to work with our 'A' and 'B' transactions. First, 'B' makes a SELECT query and lock selected rows. After, 'A' makes an INSERT query. 'B' executes a new SELECT query with the same conditions as the first one. 'B' will now see the same results as previously (the second SELECT must be made under the same transaction as the first one).",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Transactions - levels - Serializable": "this level occurs when our 'B' transaction reads the data and lock whole data's table. It means that another transaction can't modify the data on this table. Unlike read uncommited, this way is the most secure. But in the other hand, it's also the slowest solution.",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Reads": "Transaction isolation levels can lead to incoscient data reads. We can distinguish 3 types of these reads: Dirty, Phantom and NonRepeatable",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Reads - States - DirtyRead ": "caused by read uncommited level. In this inconsistency, data is different for two or more participating transactions. For example, the first transaction can read the uncommitted data by the second transaction.",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Reads - States - PhantomRead": "occurs when two identical queries, executed inside the same transaction, return different set of rows. So, it consists on reading two different set of values, inside the same transaction. But unlike dirty read, all data of phantom read are committed. Yes, it's called phantom because the first and the second SELECT queries contain some phantom data (data absent in the first SELECT results or in the second one)",
	"Querying - Transactions - Principles - Data - ACID - Isolation - Reads - States - NonRepeatableRead": "this problem looks almost as a phantom read. But unlike phantom, non-repetable read is applied the same results. Imagine that 'B' reads one row at the first time (for example, a row with id = 30) and 'A' changes one of this row's values (for example, name). After the commit of 'A' transaction, 'B' reads again the same row inside the same transaction. But regarding to the first SELECT, it retrieves the row with the different values. This error can be produced when read committed isolation level is applied to the transactions. It's called non-repeatable because the result of first read of 'B' can't be repeatead in the second read.",
	"Querying - Transactions - Principles - Data - ACID - Durability": "Transactions that have committed must survive permanently, even if the system crashes. This is usually assured by writing transactions into a log before acknowledging the commit. The log is on non-volatile storage and can be used to recreate the system state prior to failure (often automatically).",
	"Querying - Transactions - Principles - Data - BASE": ".",
	"Querying - Transactions - Principles - Data - BASE - ConsistencyModel": "BASE properties are looser than ACID. A BASE datastore values availability and scale, instead of guaranteed consistency. Used by NoSQL stores, including column family, key-value and document stores.",
	"Querying - Transactions - Principles - Data - BASE - BasicAvailability": "The database appears to work most of the time.",
	"Querying - Transactions - Principles - Data - BASE - SoftState": "Stores do not have to be write-consistent, nor do different replicas have to be mutually consistent all the time.",
	"Querying - Transactions - Principles - Data - BASE - EventualConsistency": "Stores exhibit consistency at some later point (e.g., lazily at read time).",
	"Querying - Transactions - Principles - Data - NewSQL": "https://levelup.gitconnected.com/newsql-databases-the-best-of-both-worlds-8727411a49ec",
	"Querying - Transactions - TransactionalMonitor": "",
	"Querying - Transactions - JTA": "Java Transaction API",
	"Querying - Transactions - XA": "XA is a two-phase commit protocol that is natively supported by many databases and transaction monitors.",
	"Querying - Techniques - Transactions": "https://medium.com/@patrickkoss/interview-so-how-do-you-do-a-transaction-with-nosql-databases-c3d80bc7d314",
	"Querying - Techniques - Transactions - Isolation": "https://medium.com/@patrickkoss/interview-so-how-do-you-do-a-transaction-with-nosql-databases-c3d80bc7d314",
	"Querying - Techniques - Transactions - Isolation. - States": "Isolation levels defines the degree to which a transaction must be isolated from the data modifications made by any other transaction in the database system. A transaction isolation level are defined by the following phenomena: Dirty Read, Non Repeatable read, Phantom Read",
	"Querying - Techniques - Transactions - Isolation. - States. Dirty Read": "A Dirty read is the situation when a transaction reads a data that has not yet been commited.",
	"Querying - Techniques - Transactions - Isolation. - States. Non Repeatable read": "Non Repeatable read occurs when a transaction reads same row twice, and get a different value each time. For example, suppose transaction T1 reads a data. Due to concurrency, another transaction T2 updates the same data and commit, Now if transaction T1 rereads the same data, it will retrieve a different value.",
	"Querying - Techniques - Transactions - Isolation. - States. Phantom Read": "Phantom Read occurs when two same queries are executed, but the rows retrieved by the two, are different.",
	"Querying - Techniques - Transactions - Isolation. - Levels": "The SQL standard defines four isolation levels: Read Uncommitted, Read Committed, Repeatable Read, Serializable",
	"Querying - Techniques - Transactions - Isolation. - Levels. Read Uncommitted": "Read Uncommitted is the lowest isolation level. In this level, one transaction may read not yet commited changes made by other transaction, thereby allowing dirty reads. In this level, transactions are not isolated from each other.",
	"Querying - Techniques - Transactions - Isolation. - Levels. Read Committed": "This isolation level guarantees that any data read is committed at the moment it is read. Thus it does not allows dirty read. The transaction hold a read or write lock on the current row, and thus prevent other rows from reading, updating or deleting it.",
	"Querying - Techniques - Transactions - Isolation. - Levels. Repeatable Read": "This is the most restrictive isolation level. The transaction holds read locks on all rows it references and write locks on all rows it inserts, updates, or deletes. Since other transaction cannot read, update or delete these rows, consequently it avoids non repeatable read.",
	"Querying - Techniques - Transactions - Isolation. - Levels. Serializable": "This is the Highest isolation level. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.",

	"DataOptimization": "SECTION",
	"DataOptimization - Indexing ": "A special data structure related to a database table and used for storing its important parts and enabling faster data search and retrieval. Indexes are especially efficient for large databases, where they significantly enhance query performance.",
	"DataOptimization - Indexing - Unique": "Unique index – doesn't allow duplicates in a table column and hence helps maintain data integrity.",
	"DataOptimization - Indexing - Clustered": "Clustered index – defines the physical order of records of a database table and performs data searching based on the key values. A table can have only one clustered index.",
	"DataOptimization - Indexing - Non-clustered": "Non-clustered index – keeps the order of the table records that don't match the physical order of the actual data on the disk. It means that the data is stored in one place and a non-clustered index – in another one. A table can have multiple non-clustered indexes.",
	"DataOptimization - Indexing - B-Tree": "B-trees, short for balanced trees, are the most common type of database index. A B-tree index is an ordered list of values divided into ranges. By associating a key with a row or range of rows, B-trees provide excellent retrieval performance for a wide range of queries, including exact match and range searches.",
	"DataOptimization - Indexing - Bitmap": "In a bitmap index, the database stores a bitmap for each index key. In a conventional B-tree index, one index entry points to a single row. In a bitmap index, each index key stores pointers to multiple rows. Bitmap indexes are primarily designed for data warehousing or environments in which queries reference many columns in an ad hoc fashion.",
	"DataOptimization - Managing - Archiving": "We implement archiving strategies to keep active data lean and fast, moving older data to separate storage.",
	"DataOptimization - Scaling - Replication": "We design robust replication setups to distribute read loads and enhance availability. It's like cloning your database for peak performance.",
	"DataOptimization - Scaling - Sharding": "For massive datasets, we implement sharding to divide and conquer, distributing the load across multiple servers.",
	"DataOptimization - Scaling - Partitioning": "We use partitioning to break large tables into manageable chunks, improving query performance and simplifying maintenance.",
	"DataOptimization - Design - Normalization": "Optimize for CRUDing (OLTP).",
	"DataOptimization - Design - Normalization - Features - Pros": "Better data integrity, less redundancy, easier maintenance.",
	"DataOptimization - Design - Normalization - Features - Cons": "Can slow down read operations due to multiple table joins.",
	"DataOptimization - Design - Normalization - Anomaly - Insert": "An insert anomaly occurs when you cannot add new data to the database without also adding irrelevant or redundant data. This typically happens in a poorly normalized table where some columns are dependent on others.",
	"DataOptimization - Design - Normalization - Anomaly - Delete": "A delete anomaly occurs when the deletion of a record results in the unintended loss of additional data that was associated with the record. This usually happens in a table where data is not properly normalized, causing related information to be removed inadvertently.",
	"DataOptimization - Design - Normalization - Anomaly - Update": "An update anomaly occurs when changes to data require multiple rows to be updated, leading to potential inconsistencies. This usually happens when the same piece of information is duplicated across multiple rows.",
	"DataOptimization - Design - Normalization - Forms - 1NF": "First Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Rule": "Ensure each column in a table contains unique and indivisible values.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Reason": "This eliminates repeating groups and ensures that the data in each column is atomic, meaning that it cannot be divided further. It helps in simplifying the data structure and avoids the pitfalls of storing multiple values in a single column.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Impact": "While 1NF ensures a basic level of data integrity by making data retrieval straightforward, it is not sufficient to handle more complex data dependencies and anomalies. Without further normalization, the database might still suffer from redundancy and update anomalies.",
	"DataOptimization - Design - Normalization - Forms - 2NF": "Second Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Rule": "Ensure all non-key columns are fully dependent on the entire primary key.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Reason": "This addresses the issue of partial dependency, where non-key attributes are dependent on part of a composite primary key. It ensures that each non-key attribute is only dependent on the primary key, thus eliminating redundancy for non-key attributes.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Impact": "By achieving 2NF, the database reduces redundancy and ensures that updates to the data are consistent. However, it might still face issues related to transitive dependencies, which are addressed in 3NF.",
	"DataOptimization - Design - Normalization - Forms - 3NF": "Third Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 3NF - Rule": "Remove any columns that depend on non-key columns (transitive dependency).",
	"DataOptimization - Design - Normalization - Forms - 3NF - Reason": "This ensures that all attributes are only dependent on the primary key and not on other non-key attributes. It eliminates the potential for anomalies that can occur when non-key attributes are dependent on other non-key attributes.",
	"DataOptimization - Design - Normalization - Forms - 3NF - Impact": "3NF improves data integrity by ensuring that changes in non-key attributes do not lead to inconsistencies. It also enhances query performance by reducing the number of redundant data and simplifying the schema.",
	"DataOptimization - Design - Normalization - Forms - BCNF": "Boyce-Codd Normal Form (BCNF).",
	"DataOptimization - Design - Normalization - Forms - BCNF - Rule": "A stricter version of 3NF where every determinant must be a candidate key.",
	"DataOptimization - Design - Normalization - Forms - BCNF - Reason": "BCNF addresses situations where 3NF does not fully eliminate anomalies, particularly in cases where a table has multiple overlapping candidate keys.",
	"DataOptimization - Design - Normalization - Forms - BCNF - Impact": "Achieving BCNF further strengthens data integrity by ensuring that every determinant is a candidate key, thus removing any remaining anomalies and ensuring a more robust schema.",
	"DataOptimization - Design - Normalization - Forms - 4NF": "Fourth Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Rule": "Address even more complex dependencies, such as multi-valued dependencies.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Reason": "These forms are designed to handle more complex scenarios that are not addressed by the earlier normal forms. For example, 4NF deals with multi-valued dependencies, ensuring that one attribute cannot have multiple independent multi-valued facts associated with it.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Impact": "Higher normal forms provide even more stringent controls over data integrity and further reduce redundancy, though they are less commonly applied in everyday database design.",
	"DataOptimization - Design - Normalization - Forms - 5NF": "Fifth Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 5NF - Rule": "A relation is said to be in 5FN if it is in 4FN and it cannot be further decomposed into smaller tables.",
	"DataOptimization - Design - Normalization - Forms - 5NF - Reason": ".",
	"DataOptimization - Design - Normalization - Forms - 5NF - Impact": ".",
	"DataOptimization - Design - Denormalization": "Optimize for querying (OLAP).",
	"DataOptimization - Design - Denormalization - Features - Pros": "Faster read operations, simpler queries.",
	"DataOptimization - Design - Denormalization - Features - Cons": "More redundancy, higher risk of data inconsistencies, harder maintenance.",
	"DataOptimization - Design - Denormalization - Methods - Redundant": "Adding Redundant Data: Include extra copies of data to avoid complicated lookups..",
	"DataOptimization - Design - Denormalization - Methods - Aggregates": "Creating Aggregates: Store pre-calculated summary data to speed up queries.",
	"DataOptimization - Design - Denormalization - Methods - Combining": "Combining Tables: Merge related tables to simplify queries..",

	"DataModel": "SECTION",
	"DataModel - DataStructures - View": ".",
	"DataModel - DataStructures - MaterializedView": "We pre-compute and store complex query results, reducing processing time and resource usage. Think of it as caching for your most demanding queries.",
	"DataModel - DataStructures - all": "https://medium.com/@ashishps/how-i-mastered-data-structures-and-algorithms-eb8c5273c56d",
	"DataModel - DataStructures - DataTypes": ".",
	"DataModel - Types - RBDMS": "In a 𝗿𝗲𝗹𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲, data is organized in rows and columns where a row represents a record and its data fields are stored in columns. They are ideal for when ACID compliance is required, and a predefined schema can be created.",
	"DataModel - Types - RBDMS - MySQL": "",
	"DataModel - Types - RBDMS - PostgresSQL": "",
	"DataModel - Types - Column": "With 𝗰𝗼𝗹𝘂𝗺𝗻𝗮𝗿 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀, records are stored as columns rather than rows. This makes them very performant for analytical purposes where complex queries are run across large datasets; especially those that contain aggregate functions.",
	"DataModel - Types - Column - BigQuery": "",
	"DataModel - Types - Column - Cassandra": "",
	"DataModel - Types - Document": "In a 𝗱𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲, data is stored in a semi-structured format such as JSON. They offer a flexible and schema-less approach which makes them a great choice for data with complex or continually changing structures.",
	"DataModel - Types - Document - MongoDB": "",
	"DataModel - Types - Document - CouchDB": "",
	"DataModel - Types - Graph": "𝗚𝗿𝗮𝗽𝗵 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 are optimized for storing and querying highly connected data. Records are represented as nodes and relationships as edges. Under the hood, they use graph theory to traverse relationships between nodes to power performant queries.",
	"DataModel - Types - Graph - Neo4J": "",
	"DataModel - Types - Graph - Neptune": "",
	"DataModel - Types - KeyValueStores": "𝗞𝗲𝘆-𝘃𝗮𝗹𝘂𝗲 𝘀𝘁𝗼𝗿𝗲𝘀 are a simple form of storage where values are inserted, updated, and retrieved using a unique key. They are more commonly used for small datasets and often temporary purposes such as caching or session management.",
	"DataModel - Types - KeyValueStores - Redis": "",
	"DataModel - Types - KeyValueStores - DynamoDB": "",
	"DataModel - Types - Vector": "𝗩𝗲𝗰𝘁𝗼𝗿 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 store data as vectors, enabling fast similarity searches for high-dimensional data. They are ideal for AI & ML applications, such as recommendations and searching unstructured data like images, text, or audio.",
	"DataModel - Types - Vector - Milvus": "",
	"DataModel - Types - Vector - Pinecone": "",
	"DataModel - ID - UUID": "A UUID (Universally Unique Identifier) is a 128-bit number used to uniquely identify objects or records in computer systems. There are multiple versions of UUIDs.",
	"DataModel - ID - UUID - unique": "UUIDs are not guaranteed to be unique, instead they are given a certain (typically very low) probability of collision. The probability of generating two UUIDs (especially v4 or v7) that collide is very low, but not zero.",
	"DataModel - ID - UUIDv1 - Time-based": "A combination of the current timestamp and the MAC address of the machine, unique but may expose hardware information.",
	"DataModel - ID - UUIDv2 - DCE Security": "Similar to Version 1 but includes POSIX UID/GID information, for applications requiring user or group identification.",
	"DataModel - ID - UUIDv3 - Name-based, MD5": "Hashes a namespace identifier and a name using the MD5 algorithm, producing consistent UUIDs for the same input data.",
	"DataModel - ID - UUIDv4 - Random": "Uses random numbers, offering simplicity and a low probability of duplication. e942bbe9-afdc-4c62-a438-4efee77954b3 You can tell it’s UUIDv4 because the digit ‘4’ appears in the 13th position. This is a key identifier for the version of the UUID.",
	"DataModel - ID - UUIDv5 - Name-based, SHA-1": "Similar to Version 3 but uses the SHA-1 hashing algorithm, providing a more secure hash function for generating UUIDs from names.",
	"DataModel - ID - UUIDv6 - Ordered Time-based": "A reordering of Version 1 UUIDs to improve database indexing by placing the timestamp in the most significant bits, facilitating chronological ordering. May be suitable for some use cases of database keys needing ordering.",
	"DataModel - ID - UUIDv7 - Unix Epoch Timestamp": "Encodes a Unix timestamp with millisecond precision in the most significant 48 bits, followed by random data, ensuring uniqueness and time-ordering. 01922e13-43f2-79ef-ab97-ca7a7d021d34 UUIDv7 is a time-based version of UUID, meaning that the identifiers are generated in an increasing order.",
	"DataModel - ID - UUIDv7 - ordered": "Because the UUIDs are time-based, they are ordered, which leads to better indexing performance compared to the randomness of UUIDv4.",
	"DataModel - ID - UUIDv8 - Custom": "Reserved for custom implementations, allowing for the inclusion of application-specific data within the UUID structure.",
	"DataModel - ID - ULID": "Universally Unique Lexicographically Sortable Identifier. ULID is a UUID alternative that is also globally unique, but with the added benefit of being lexicographically sortable.",
	"DataModel - ID - Auto-IncrementingIDs": ".",
	"DataModel - ID - SnowflakeID": "Twitter. A distributed ID generation algorithm that generates 64-bit unique IDs using a combination of a timestamp, machine ID, and sequence number.",
	"DataModel - ID - KSUID": "KSUID (K-Sortable Unique Identifier). KSUIDs are a variation of UUIDs that include a timestamp, making them sortable by creation time. A 27-character string consisting of a timestamp and randomly generated bits, ensuring k-sortability.",
	"DataModel - ID - NanoID": "NanoID is a small, fast, and secure alternative to UUID, designed to be URL-friendly and customizable in terms of size. A short, random, URL-friendly string with customizable length and alphabet.",
	"DataModel - ID - RandomHash-BasedID": "Random Hash-Based ID (SHA-256 or MD5 Hashing). Randomly generated strings using hash functions like SHA-256 or MD5 to create unique identifiers. A fixed-length 32- or 64-character string generated by hashing data (like a combination of timestamp and user data).",
	"DataModel - ID - ObjectID": "ObjectID (MongoDB ObjectID). MongoDB’s ObjectID (BSON binary JSON) is a 12-byte unique identifier that includes a timestamp, machine identifier, process identifier, and a counter.",
	"DataModel - ID - CUID2": "CUID2 (Collision-Resistant Unique Identifier). Cuid2 is designed to minimize the likelihood of collision in distributed systems, providing a URL-safe, human-readable, and collision-resistant ID.",
	"DataModel - ID - FlakeID": "Includes a timestamp, machine identifier, and sequence number. If you require unique, time-ordered identifiers for easier sorting and debugging. Example: 304857642123456",
	"DataModel - ID - Time-basedIDs": "Often a Unix timestamp concatenated with random bits or other unique data. Where chronological order is important, such as logging or event tracking.",
	"DataModel - ID - SequentialGUIDs": "A GUID optimized for indexing, often beginning with a time-ordered segment. Where GUIDs are necessary, but ordered indexing is needed for better database performance. Example: 6E4F6A80-4F64-11EE-B4FA-0242AC120002",
	"DataModel - ID - ShortID": "Generates a compact, unique alphanumeric string, often used in URLs. URL shortening or user-friendly identifiers in public URLs. Example: 2K5czP8",
	"DataModel - ID - ZUID": "ZUID (Zero-width Unique Identifier): Involves zero-width characters (e.g., zero-width spaces) that are invisible but can be parsed for uniqueness. If you need invisible identifiers for tracking or metadata without impacting visual layout. Example: Internally may look like \u200B\u200C\u200D"

}