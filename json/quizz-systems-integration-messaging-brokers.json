{

	"PushPull": "SECTION",
	"PushPull 0": "https://medium.com/@kanishksinghpujari/what-happens-if-producers-send-messages-faster-than-consumers-can-process-them-c3263c126e60",

	"JMS": "SECTION",
	"JMS - Components - Provider": "",
	"JMS - Components - Provider - Connection": "",
	"JMS - Components - Provider - Session": "",
	"JMS - Components - Provider - Transaction": "",
	"JMS - Components - Provider - Sending": "",
	"JMS - Components - Provider - Sending - Sync": "",
	"JMS - Components - Provider - Sending - Async": "",
	"JMS - Components - Provider - Sending - Order": "",
	"JMS - Components - Provider - Sending - Duplicated": "",
	"JMS - Components - Provider - Sending - Concurrency": "",
  "JMS - Components - Broker - Message": ".",
  "JMS - Components - Broker - Message - Header": ".",
  "JMS - Components - Broker - Message - Properties": ".",
  "JMS - Components - Broker - Message - Body": ".",
	"JMS - Components - Broker - Queues": "",
	"JMS - Components - Broker - Queues - P2P": "",
	"JMS - Components - Broker - Topics": "Topics (sometimes also referred to as channels) are key to Publish-Subscribe systems - they are used to broadcast information to all subscribers.",
	"JMS - Components - Broker - Topics - PubSub": "",
	"JMS - Components - Consumer": "",
	"JMS - Components - Consumer - Queue": "",
	"JMS - Components - Consumer - Topic": "",
	"JMS - Components - Consumer - Receiving": "",
	"JMS - Components - Consumer - Receiving - Sync": "",
	"JMS - Components - Consumer - Receiving - Async": "",

	"WebSphereMQ": "SECTION",
  "WebSphereMQ - Features - 1993": "WebSphere MQ, initially named MQSeries, was first launched by IBM in December 1993. It was later renamed to WebSphere MQ in 2002 to align with the broader WebSphere suite. ",
  "WebSphereMQ - Features - MesageModel": "Queue-topic based",
  "WebSphereMQ - Features - Persistence": "Yes, strong durability",
  "WebSphereMQ - Features - Performance": "Moderate to high",

	"ActiveMQ": "SECTION",
  "ActiveMQ - Features": ".",
  "ActiveMQ - Features - 2004": "The ActiveMQ project was originally created by its founders from LogicBlaze in 2004, as an open source message broker, hosted by CodeHaus. The code and ActiveMQ trademark were donated to the Apache Software Foundation in 2007, where the founders continued to develop the codebase with the extended Apache community.",
  "ActiveMQ - Features - MesageModel": "Queue-topic based",
  "ActiveMQ - Features - Persistence": "Yes",
  "ActiveMQ - Features - Performance": "Moderate",
  "ActiveMQ - Usage - OptimizedFor": "The Enterprise-Ready Solution Best for: Protocol-heavy enterprise systems and legacy integrations. Go with ActiveMQ for robust, enterprise-level messaging needs.",
  "ActiveMQ - Usage - Strength": "Supports a variety of messaging protocols (AMQP, MQTT, JMS). Rich support for traditional patterns like point-to-point and publish/subscribe. Reliable and proven in enterprise environments.",
  "ActiveMQ - Usage - UseCase": "Integrating legacy systems with modern apps or cross-protocol communication.",

  "RabbitMQ": "SECTION",
  "RabbitMQ 0": "https://www.linkedin.com/posts/rushikesh-patil-3a37051b5_implementing-rabbitmq-in-spring-mvc-a-hands-on-activity-7329366444582797312-GpSc/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "RabbitMQ 1": "https://www.linkedin.com/posts/abdul-moiz-411615321_what-is-rabbitmq-activity-7332744894559485952-c0We?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "RabbitMQ 2": "https://www.linkedin.com/posts/dhirendra-patel-20966a176_servicecommunication-activity-7334487114551631875-6VFz?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "RabbitMQ 3": "",
  "RabbitMQ 4": "",
  "RabbitMQ - Features": "It was developed by Rabbit Technologies Ltd., which was a joint venture between LShift and CohesiveFT. It is now a Free and Open Source Software (FOSS). ",
  "RabbitMQ - Features - 2007": "RabbitMQ was originally released in 2007.",
  "RabbitMQ - Features - MesageModel": "Queue based (push)",
  "RabbitMQ - Features - Persistence": "Yes, durable queues",
  "RabbitMQ - Features - Reliability": "Supports message acknowledgment, enabling reliable message delivery",
  "RabbitMQ - Features - Performance": "Moderate",
  "RabbitMQ - Features - Protocols": "Built on the AMQP protocol. Supports AMQP, MQTT, STOMP",
  "RabbitMQ - Usage": "lightweight messaging and quick integrations.",
  "RabbitMQ - Usage - OptimizedFor": "The Lightweight and Versatile Choice. Best for: Task queues, real-time messaging, and microservices communication.",
  "RabbitMQ - Usage - Strength": "Flexible routing, and prioritization. Easy setup and integration across multiple languages.",
  "RabbitMQ - Usage - UseCase:": "Asynchronous workflows like email notifications or order processing.",

	"ZeroMQ": "SECTION",
  "ZeroMQ - Features": "",
  "ZeroMQ - Features - 2010": "On March 30, 2010, Hintjens announced that iMatix (the original designer of Advanced Message Queuing Protocol) would leave the AMQP workgroup and did not plan to support AMQP/1.0 in favor of the significantly simpler and faster ZeroMQ.[9][10]",

  "Kafka": "SECTION",
  "Kafka 0": "https://www.linkedin.com/posts/shivam-baghel-sde_kafka-activity-7318349982267285504-SH8a/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 1": "https://www.linkedin.com/posts/sraban-kumar-jena-10a5b8324_java-opportunity-frontends-activity-7313239221811503108-5fNi/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 2": "https://www.linkedin.com/posts/aurimas-griciunas_dataengineering-data-activity-7313138917363400706-lkRy?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 3": "https://www.linkedin.com/posts/piyush-ranjan-9297a632_rabbitmq-vs-kafka-which-one-should-you-activity-7313893111296905217-lgPb/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 4": "https://www.linkedin.com/posts/sina-riyahi_top-10-message-broker-google-cloud-pub-activity-7312373700396261378-zHyw/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 5": "https://www.linkedin.com/posts/satya619_%F0%9D%97%A8%F0%9D%97%BB%F0%9D%97%B1%F0%9D%97%B2%F0%9D%97%BF%F0%9D%98%80%F0%9D%98%81%F0%9D%97%AE%F0%9D%97%BB%F0%9D%97%B1%F0%9D%97%B6%F0%9D%97%BB%F0%9D%97%B4-%F0%9D%98%81%F0%9D%97%B5%F0%9D%97%B2-%F0%9D%97%9E%F0%9D%97%AE%F0%9D%97%B3%F0%9D%97%B8%F0%9D%97%AE-activity-7311598926082539521-E_N1?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 6": "https://www.linkedin.com/posts/pooja-jain-898253106_kafka-datamining-data-activity-7312541697622556672-jbuE?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 7": "https://www.linkedin.com/posts/stanislavkozlovski_kafka-activity-7305595851609931776-iV-3?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 8": "https://www.linkedin.com/posts/brijpandeyji_most-people-see-kafka-as-a-pub-sub-system-activity-7295637611618926593-7P6y?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka 9": "https://www.linkedin.com/posts/bytebytego_systemdesign-coding-interviewtips-activity-7295678436893708288-U7CL?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka10": "https://www.linkedin.com/posts/stanislavkozlovski_tiered-storage-read-path-activity-7324409267900370944-pUKu/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka11": "https://medium.com/@kanishksinghpujari/kafka-vs-rabbitmq-i-tried-both-so-you-dont-have-to-355f5ed04d9b",
  "Kafka12": "https://www.linkedin.com/posts/piyush-ranjan-9297a632_rabbitmq-vs-kafka-which-one-should-you-activity-7333337018891988993-vnLH/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka13": "https://www.linkedin.com/posts/abhisek-sahu-84a404b1_kafka-engineering-data-activity-7337833984904585216-s1rl?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka14": "https://www.linkedin.com/posts/brijpandeyji_%F0%9D%97%AA%F0%9D%97%B5%F0%9D%97%AE%F0%9D%98%81-%F0%9D%97%B6%F0%9D%98%80-%F0%9D%97%97%F0%9D%97%B6%F0%9D%98%80%F0%9D%97%B8%F0%9D%97%B9%F0%9D%97%B2%F0%9D%98%80%F0%9D%98%80-%F0%9D%97%9E%F0%9D%97%AE%F0%9D%97%B3%F0%9D%97%B8%F0%9D%97%AE-activity-7339144985356767232-A5i5?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
  "Kafka15": "",
  "Kafka16": "",
  "Kafka17": "",
  "Kafka18": "",
  "Kafka19": "",
  "Kafka20": "",
	"Kafka - Features": "Apache Kafka is developed in Scala and started out at LinkedIn.",
  "Kafka - Features - 2010": "Kafka was initially developed internally at LinkedIn in 2010 and released as an open-source project in January 2011.",
  "Kafka - Features - MesageModel": "Log based (pull)",
  "Kafka - Features - Persistence": "Yes, log storage and retention",
  "Kafka - Features - Scalable": "Horizontally scalable.",
  "Kafka - Features - Performance": "High because Sequential I/O and Zero-copy Optimization",
  "Kafka - Features - Performance - SequentialIO": "Instead of scattering writes across the disk, Kafka writes data sequentially. This drastically reduces seek time and takes advantage of how disks (even SSDs) are optimized for linear access.",
  "Kafka - Features - Performance - ZeroCopyOptimization": "Normally, data moves through several memory layers before reaching the network. Kafka skips most of that by using sendfile(), which transfers data directly from the disk to the network card, no need to load it into Kafka’s memory space. Less CPU, less memory movement, more throughput. Here’s how the data flow looks: ▪️Without zero-copy: Disk → OS cache → Kafka app → socket buffer → network card → consumer ▪️ With zero-copy: Disk → OS cache → sendfile() → network card → consumer",
	"Kafka - Features - KIP": "KIPs (Kafka Improvement Proposals)",
	"Kafka - Components - Producer": "Producers in Kafka create new messages, batch them, and send them to a Kafka topic.",
	"Kafka - Components - Producer - Tuning": "Set acks=all, tweak linger.ms & batch.size for efficiency.",
	"Kafka - Components - Producer - Idempotent": "Enable enable.idempotence=true for exactly-once delivery.",
	"Kafka - Components - Producer - Ackhnowledgements": "",
	"Kafka - Components - Producer - Ackhnowledgements - acks=0": "Fire-and-forget (no guarantee).",
	"Kafka - Components - Producer - Ackhnowledgements - acks=1": "Leader acknowledgment.",
	"Kafka - Components - Producer - Ackhnowledgements - acks=11": "Leader + replicas acknowledgment (strongest guarantee).",
	"Kafka - Components - Producer - KafkaConnect": "Kafka Connect allows you to continuously ingest data from external systems into Kafka, and vice versa.",
	"Kafka - Components - Schema": "Schemas reside outside of your Kafka cluster, only the schema ID resides in your Kafka, hence making schema registry a critical component of your infrastructure. If the schema registry is not available, it will break producers and consumers. So it is always a best practice to ensure your schema registry is highly available.",
	"Kafka - Components - Schema - Operation": ".",
	"Kafka - Components - Schema - Operation - necessity": "Kafka, at its core, only transfers data in byte format. There is no data verification that’s being done at the Kafka cluster level. In fact, Kafka doesn’t even know what kind of data it is sending or receiving; whether it is a string or integer.",
	"Kafka - Components - Schema - Operation - Steps - check": "With the schema registry in place, the producer, before sending the data to Kafka, talks to the schema registry first and checks if the schema is available.",
	"Kafka - Components - Schema - Operation - Steps - register": "If it doesn’t find the schema then it registers and caches it in the schema registry.",
	"Kafka - Components - Schema - Operation - Steps - serialize": "Once the producer gets the schema, it will serialize the data with the schema and send it to Kafka in binary format prepended with a unique schema ID.",
	"Kafka - Components - Schema - Operation - Steps - match": "When the consumer processes this message, it will communicate with the schema registry using the schema ID it got from the producer and deserialize it using the same schema.",
	"Kafka - Components - Schema - Operation - Steps - mismatch": "If there is a schema mismatch, the schema registry will throw an error letting the producer know that it’s breaking the schema agreement.",
	"Kafka - Components - Schema - Operation - Steps - deserialize": "consumer deserialize message using the schema.",
	"Kafka - Components - Schema - Location": "Schemas reside outside of your Kafka cluster, only the schema ID resides in your Kafka, hence making schema registry a critical component of your infrastructure. If the schema registry is not available, it will break producers and consumers. So it is always a best practice to ensure your schema registry is highly available.",
	"Kafka - Components - Schema - SchemaRegistry": "resides outside of your Kafka cluster and handles the distribution of schemas to the producer and consumer by storing a copy of schema in its local cache.",
	"Kafka - Components - Schema - SchemaRegistry - How": "With the schema registry in place, the producer, before sending the data to Kafka, talks to the schema registry first and checks if the schema is available. If it doesn’t find the schema then it registers and caches it in the schema registry. Once the producer gets the schema, it will serialize the data with the schema and send it to Kafka in binary format prepended with a unique schema ID. When the consumer processes this message, it will communicate with the schema registry using the schema ID it got from the producer and deserialize it using the same schema. If there is a schema mismatch, the schema registry will throw an error letting the producer know that it’s breaking the schema agreement.",
  "Kafka - Components - Message": "Message is the basic unit of data in Kafka. It consists of headers, key, and value.",
  "Kafka - Components - Message - Formats": ".",
  "Kafka - Components - Message - Formats - JSON": ".",
  "Kafka - Components - Message - Formats - XML": ".",
  "Kafka - Components - Message - Formats - YAML": ".",
  "Kafka - Components - Message - Formats - AVRO": "Avro is an open-source binary data serialization format that comes from the Hadoop world.",
  "Kafka - Components - Message - Formats - ProtocolBuffer": ".",
  "Kafka - Components - Message - Formats - Thrift": ".",
  "Kafka - Components - Message - Ordering": ".",
  "Kafka - Components - Message - Delivery": ".",
  "Kafka - Components - Message - Delivery - Semantics - at-least-once": ".",
  "Kafka - Components - Message - Delivery - Semantics - exactly-once": ".",
  "Kafka - Components - Message - Delivery - Semantics - at-most-once": ".",
	"Kafka - Components - Broker": "Store and distribute data across servers.",
  "Kafka - Components - Broker - Types - Leader": "handling reads/writes",
  "Kafka - Components - Broker - Types - Followers": "replicating data",
  "Kafka - Components - Broker - Queues": "Consumer Groups allow Kafka to behave like a Queue, since each consumer instance in a group processes data from a non-overlapping set of partitions (within a Kafka topic).",
  "Kafka - Components - Broker - Topics": "Messages are organized into topics. Every message goes to a particular Topic.",
  "Kafka - Components - Broker - Topics - Compacted": "Retains only the latest value for each key (useful for stateful applications).",
  "Kafka - Components - Broker - Topics - Lifecycle - Creation - kafka-topics.sh": "Topics are created using CLI tools (kafka-topics.sh), APIs, or auto-created (if configured).",
	"Kafka - Components - Broker - Topics - Partitions": "The core unit of storage and distribution in Kafka. A Kafka topic is further divided into units called partitions. Each partition is an ordered, immutable sequence of messages. Partitions allow topics to be split across multiple brokers for scalability.",
  "Kafka - Components - Broker - Topics - Partitions - Log": "Kafka stores messages in a log-structured format on disk.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Segments": "Partitions are stored as a series of files called log segments.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Segments - Immutable": "These segments are immutable—once written, they don’t change—and new messages are appended to the active segment.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Segments - Size": "When a segment reaches a size or time limit (configurable), it’s closed, and a new segment is created.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Segments - Retain": "Older segments are retained based on retention policies (e.g., time or size limits) and eventually deleted when no longer needed.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Offset": "The unique ID of each message. Since offset 100 could reside in any segment of a partition, the broker must efficiently locate it without scanning every message from the beginning.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Offset - Index": "To enable fast message lookups, Kafka maintains an offset index for each partition.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Offset - Index - Structure - Location": "Offsets (e.g., 100, 110, 120, etc.) → Physical location in the log (i.e., which segment file and position within that file the message is stored).",
  "Kafka - Components - Broker - Topics - Partitions - Log - Offset - Index - Structure - Entries": "The index doesn’t map every single offset (to avoid inefficiency); instead, it records entries periodically (e.g., every 4 KB of data, configurable via index.interval.bytes).",
  "Kafka - Components - Broker - Topics - Partitions - Log - Offset - Locating": "𝙒𝙝𝙚𝙣 𝙖 𝙘𝙤𝙣𝙨𝙪𝙢𝙚𝙧 𝙧𝙚𝙦𝙪𝙚𝙨𝙩𝙨 𝙤𝙛𝙛𝙨𝙚𝙩 100, 𝙩𝙝𝙚 𝙗𝙧𝙤𝙠𝙚𝙧, • Checks the offset index for the closest indexed offset ≤ 100. • Jumps to the corresponding segment file and position. • Scans sequentially from there to reach offset 100.",
  "Kafka - Components - Broker - Topics - Partitions - Log - Compaction": "Kafka’s log compaction is a retention mechanism that ensures the log (topic) retains only the latest value for each key, while still preserving a complete history of changes for a defined period. It is particularly useful for stateful data where only the current state matters, rather than every intermediate update.",
	"Kafka - Components - Broker - Topics - Partitions - LocalStorage": "Each partition represents a portion of the topic’s data and is stored in a single log file.",
	"Kafka - Components - Broker - Topics - Partitions - LocalStorage - Location": "The storage is a disk attached to the broker. This can be HDD or SDD disks on-premise or e.g. EBS volumes on AWS cloud.",
	"Kafka - Components - Broker - Topics - Partitions - LocalStorage - Populate": "Messages assigned to partitions By key (if specified) or round-robin (if no key).",
	"Kafka - Components - Broker - Topics - Partitions - LocalStorage - Scalability": "Partitioning enables parallel message distribution among several brokers in the cluster, facilitating scalability for consumers and producer",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy": "Adjust log.retention.hours & segment.bytes wisely.",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy - Inmutable": "Once written, messages cannot be modified or deleted (except via retention policies or compaction).",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy - Ordered": "Messages in a partition are stored in the order they arrive and are assigned an incremental offset (unique ID). Order is guaranteed only within a single partition, not across the entire topic.",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy - Durability": "Kafka replicates partitions across multiple brokers (servers) to ensure durability and high availability.",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy - FaultTolerance": ".",
  "Kafka - Components - Broker - Topics - Partitions - LocalStorage - RetentionPolicy - ConfigurableRetention": "Topics can retain messages based on time (e.g., 7 days) or size (e.g., 1 GB). Log compaction can also be enabled to retain only the latest value for each key.",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - KIP 405": "Besides the disks attached to the broker, Kafka offloads data to an external storage. Most times, this is an object storage like Amazon S3, Azure Blog Storage, Google Cloud Storage, or MinIO for Kubernetes.",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - Implementation": "Tiered Storage for Apache Kafka is available. However, be aware that different implementations exist with different features, maturity, and support levels.",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - Implementation - Interface": "And open source Apache Kafka only provides the interface for tiered storage. You must choose an open source implementation, build your own integration into an external storage system, or leverage a commercial product or cloud service that embeds tiered storage into its offering.",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - Implementation - Producers": "There are no API or code changes needed in Kafka client applications. Hence, you can easily migrate an existing deployment to a Kafka cluster leveraging Tiered Storage.",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - Implementation - Consumers": "Kafka consumers do not see the implementation details of Kafka’s Tiered Storage. They just consumed as if there was no tiered storage implementation (and still expect the same behavior).",
	"Kafka - Components - Broker - Topics - Partitions - TieredStorage - DisklessTopics - KIP1150": "𝗞𝗜𝗣-𝟭𝟭𝟱𝟬: 𝗗𝗶𝘀𝗸𝗹𝗲𝘀𝘀 𝗧𝗼𝗽𝗶𝗰𝘀 proposes a foundational shift, offering a way to write Kafka topic data directly to 𝗼𝗯𝗷𝗲𝗰𝘁 𝘀𝘁𝗼𝗿𝗮𝗴𝗲( 𝗦𝟯, 𝗚𝗼𝗼𝗴𝗹𝗲 𝗔𝘇𝘂𝗿𝗲 𝗕𝗹𝗼𝗯 𝗦𝘁𝗼𝗿𝗮𝗴𝗲, 𝗲𝘁𝗰.)  instead of relying on local disks.",
	"Kafka - Components - Broker - Cluster": "A Kafka cluster consists of several brokers where each partition is replicated across multiple brokers to provide high availability and redundancy.",
	"Kafka - Components - Broker - Cluster - Node": ".",
	"Kafka - Components - Broker - Cluster - Node - Replica": "Replication is for replicating nodes within a Kafka cluster. ensure fault tolerance",
	"Kafka - Components - Broker - Cluster - Node - Replica - InSyncReplicas": "A set of replicas (leader + followers) that are fully caught up with the leader. Only ISR members can become leaders.",
	"Kafka - Components - Broker - Cluster - Manage - ZooKeeper": "In Apache Kafka, ZooKeeper, a distributed coordination service, manages cluster metadata, broker registration, leader election, and topic/partition information, ensuring the reliability and coordination of the Kafka cluster.",
	"Kafka - Components - Broker - Cluster - Manage - ZooKeeper - deprecated": "its role has been reduced significantly in newer versions (with plans to fully remove it in Kafka 4.0 via KIP-500)",
	"Kafka - Components - Broker - Cluster - Manage - KRaft": "With KIP-500, Kafka introduced self-managed metadata (KRaft mode), making ZooKeeper optional (and eventually obsolete).",
	"Kafka - Components - Broker - Cluster - Mirroring": "Mirroring is replicating data between Kafka cluster",
	"Kafka - Components - Broker - Cluster - Mirroring - Tools - MirrorMaker": "Kafka MirrorMaker is a tool provided by Apache Kafka for replicating (mirroring) data between two or more separate Kafka clusters. It acts as a consumer-producer bridge, reading messages from a source cluster and writing them to a target cluster.",
	"Kafka - Components - Broker - Cluster - Mirroring - Tools - MirrorMaker2": "Built on Kafka Connect. Use MirrorMaker2 (MM2) for most cases—it’s more robust and feature-rich.",
	"Kafka - Components - Broker - Cluster - Mirroring - Tools - ConfluentReplicator": "Enterprise tool with better monitoring.",
	"Kafka - Components - Broker - Cluster - Mirroring - Tools - UberuReplicator": "Optimized for large-scale mirroring.",
	"Kafka - Components - Broker - Cluster - Mirroring - Tools - Custom": ".",
	"Kafka - Components - Consumer": "Kafka consumers work together as a consumer group to read messages from the broker. ",
	"Kafka - Components - Consumer - Balanced": "Ensure even consumer distribution across partitions.",
	"Kafka - Components - Consumer - Group": "A set of consumers that work together.",
	"Kafka - Components - Consumer - Group - partition": "In a consumer group, each partition is consumed by exactly one consumer.",
	"Kafka - Components - Consumer - Group - parallel": "More partitions = more consumers can work in parallel (scaling consumption).",
	"Kafka - Components - Consumer - Tuning": ".",
	"Kafka - Components - KafkaStreams": "Kafka Streams is a lightweight library developed by Apache Kafka that enables real-time stream processing of data stored in Kafka topics.",
  "Kafka - Usage - OptimizedFor": "The Real-Time Data Streamer. Best for: High-throughput event streaming, big data pipelines, and log aggregation. Pick Kafka for large-scale, real-time event-driven systems.",
  "Kafka - Usage - Strength": "Distributed and partitioned for scalability and fault-tolerance. Optimized for event-driven architectures and real-time analytics. Includes stream processing capabilities via Kafka Streams and KSQL.",
  "Kafka - Usage - Advantages": "Kafka can handle multiple producers and consumers, while providing disk-based data retention and high scalability. ",
	"Kafka - Usage - UseCase": "Streaming IoT data or powering real-time financial dashboards.",
	"Kafka - Usage - UseCase - RealTime": "Sending and receiving real-time data.",
	"Kafka - Usage - UseCase - RealTime - EDA": ".",
	"Kafka - Usage - UseCase - RealTime - LogProcessing": "Log Processing and Analysis: Efficiently handles massive volumes of log data for analysis and insight generation.",
	"Kafka - Usage - UseCase - RealTime - DataStreaming": "Data Streaming for Recommendations: Powers real-time data streaming to deliver personalized recommendations.",
	"Kafka - Usage - UseCase - RealTime - SystemMonitoring": "System Monitoring and Alerting: Facilitates real-time monitoring and alerting systems for timely responses to system metrics.",
	"Kafka - Usage - UseCase - RealTime - ChangeDataCapture": "Change Data Capture (CDC): Captures and processes database changes to keep data in sync across systems.",
	"Kafka - Usage - UseCase - RealTime - SystemMigration": "System Migration: Supports the seamless migration of systems by ensuring data consistency and availability.",
	"Kafka - Usage - UseCase - RealTime - MonitorLag": "Use Kafka Manager or Prometheus to track consumer lag.",
	"Kafka - Usage - UseCase - RealTime - StreamProcessing": "Use Kafka Streams or Flink for real-time processing.",
	"Kafka - Usage - UseCase - Storage": "Storing data across multiple servers.",

  "Kafka - Forks": "SECTION",
	"Kafka - Forks - WarpStream": "WarpStream reimplemented the Apache Kafka protocol, utilizing S3 object storage to build Kafka functionalities. Their architecture does not reuse any Kafka code but instead uses the WarpStream Agent component to reimplement Kafka features and APIs. Messages produced by applications are directly forwarded to S3 object storage by the Agent, which itself does not need to implement storage logic, resulting in a completely stateless architectural design. The stateless design allows WarpStream agents to scale without caring about the underlying data (data rebalancing). The agents operate within the customer’s VPC (BYOC), ensuring data privacy. Only metadata is transmitted back to the WarpStream cloud service.",
	"Kafka - Forks - AutoMQ": "AutoMQ, adopted a stateless design philosophy with storage-compute separation in its technical architecture. It reuses the Apache Kafka computation layer code. It swaps the local disk-based storage layer with remote storage based on S3 object storage. Only minimal aspects of the storage layer are replaced, maintaining the consistency of the upper-layer protocol and functional interfaces.",
	"Kafka - Forks - Bufstream": "",
	"Kafka - Forks - Bufstream ": "Protobuf + Kafka https://www.linkedin.com/posts/nikkisiapno_kafka-vs-bufstream-whats-the-difference-activity-7307343842318462976-Agz1/",
  "Kafka - Forks - Bufstream - Features": "Bufstream is the Kafka-compatible message queue built for the data lakehouse era. It’s a drop-in replacement for Apache Kafka, but instead of requiring expensive machines with large attached disks, Bufstream builds on top of off-the-shelf technologies such as S3 and Postgres to provide a Kafka implementation.",
  "Kafka - Forks - Bufstream - Features - MesageModel": "",

	"NATS": "SECTION",
  "NATS - Features - 2011": "The initial release of the NATS BROKER was in 2011. The NATS project was donated by Synadia Communications and is primarily developed by them. The NATS server is written in the Go programming language.",
  "NATS - Features - MesageModel": "Subject-based (pub-sub)",
  "NATS - Features - Persistence": "Optionally, memory first with persistance",
  "NATS - Features - Performance": "Extremely fast and light weight",

  "MQTT - Brokers": "SECTION",
	"MQTT - Brokers - IBM IoT MessageSight": "",
	"MQTT - Brokers - IBM WebSphere MQ Telemetry": "",
  "MQTT - Brokers - Mosquito": "",
	"MQTT - Brokers - VerneMQ": "",
	"MQTT - Brokers - HiveMQ": "",
	"MQTT - Brokers - HerokuCloudMQTT": "",
	"MQTT - Brokers - ThingMQ": "",
	"MQTT - Brokers - MachineHead": "",
	"MQTT - Brokers - Pushy": "",

	"Sockets - Brokers": "SECTION",
	"Sockets - Mercury": "This is a message broker that enables some common messaging patterns over WebSockets.",

	"Cloud - Brokers": "SECTION",
	"Cloud - Brokers - AWS SQS": "",
	"Cloud - Brokers - AWS SNS": "",
	"Cloud - Brokers - AWS AmazonMQ": "",
	"Cloud - Brokers - MicrosoftAzureEventHubs": "",
	"Cloud - Brokers - MicrosoftAzureIoT": "MQTT",
	"Cloud - Brokers - Google pub/sub": ""

}