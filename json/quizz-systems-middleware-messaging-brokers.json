{

	"JMS": "SECTION",
	"JMS - Components - Provider": "",
	"JMS - Components - Provider - Connection": "",
	"JMS - Components - Provider - Session": "",
	"JMS - Components - Provider - Transaction": "",
	"JMS - Components - Provider - Sending": "",
	"JMS - Components - Provider - Sending - Sync": "",
	"JMS - Components - Provider - Sending - Async": "",
	"JMS - Components - Provider - Sending - Order": "",
	"JMS - Components - Provider - Sending - Duplicated": "",
	"JMS - Components - Provider - Sending - Concurrency": "",
  "JMS - Components - Broker - Message": ".",
  "JMS - Components - Broker - Message - Header": ".",
  "JMS - Components - Broker - Message - Properties": ".",
  "JMS - Components - Broker - Message - Body": ".",
	"JMS - Components - Broker - Queues": "",
	"JMS - Components - Broker - Queues - P2P": "",
	"JMS - Components - Broker - Topics": "Topics (sometimes also referred to as channels) are key to Publish-Subscribe systems - they are used to broadcast information to all subscribers.",
	"JMS - Components - Broker - Topics - PubSub": "",
	"JMS - Components - Consumer": "",
	"JMS - Components - Consumer - Queue": "",
	"JMS - Components - Consumer - Topic": "",
	"JMS - Components - Consumer - Receiving": "",
	"JMS - Components - Consumer - Receiving - Sync": "",
	"JMS - Components - Consumer - Receiving - Async": "",

	"WebSphereMQ": "SECTION",
  "WebSphereMQ - Features - MesageModel": "Queue-topic based",
  "WebSphereMQ - Features - Persistence": "Yes, strong durability",
  "WebSphereMQ - Features - Performance": "Moderate to high",

  "RabbitMQ": "SECTION",
  "RabbitMQ - Features - MesageModel": "Queue based (push)",
  "RabbitMQ - Features - Persistence": "Yes, durable queues",
  "RabbitMQ - Features - Performance": "Moderate",
  "RabbitMQ - Usage - OptimizedFor": "The Lightweight and Versatile Choice. Best for: Task queues, real-time messaging, and microservices communication. Choose RabbitMQ for lightweight messaging and quick integrations.",
  "RabbitMQ - Usage - Strength": "Built on the AMQP protocol, enabling reliable message delivery. Supports message acknowledgment, flexible routing, and prioritization. Easy setup and integration across multiple languages.",
  "RabbitMQ - Usage - UseCase:": "Asynchronous workflows like email notifications or order processing.",

  "ActiveMQ": "SECTION",
  "ActiveMQ - Features - MesageModel": "Queue-topic based",
  "ActiveMQ - Features - Persistence": "Yes",
  "ActiveMQ - Features - Performance": "Moderate",
  "ActiveMQ - Usage - OptimizedFor": "The Enterprise-Ready Solution Best for: Protocol-heavy enterprise systems and legacy integrations. Go with ActiveMQ for robust, enterprise-level messaging needs.",
  "ActiveMQ - Usage - Strength": "Supports a variety of messaging protocols (AMQP, MQTT, JMS). Rich support for traditional patterns like point-to-point and publish/subscribe. Reliable and proven in enterprise environments.",
  "ActiveMQ - Usage - UseCase": "Integrating legacy systems with modern apps or cross-protocol communication.",

  "Kafka": "SECTION",
  "Kafka - Features - MesageModel": "Log based (pull)",
  "Kafka - Features - Persistence": "Yes, log storage and retention",
  "Kafka - Features - Performance": "High",
	"Kafka - Features": "Apache Kafka is developed in Scala and started out at LinkedIn. Kafka only provides a total order over messages within a partition, not between different partitions in a topic.",
  "Kafka - Usage - OptimizedFor": "The Real-Time Data Streamer. Best for: High-throughput event streaming, big data pipelines, and log aggregation. Pick Kafka for large-scale, real-time event-driven systems.",
  "Kafka - Usage - Strength": "Distributed and partitioned for scalability and fault-tolerance. Optimized for event-driven architectures and real-time analytics. Includes stream processing capabilities via Kafka Streams and KSQL.",
  "Kafka - Usage - Advantages": "Kafka can handle multiple producers and consumers, while providing disk-based data retention and high scalability. ",
	"Kafka - Usage - UseCase": "Streaming IoT data or powering real-time financial dashboards.",
	"Kafka - Usage - UseCase - Messaging": "Sending and receiving real-time data.",
	"Kafka - Usage - UseCase - Streaming": "Processing data as it arrives.",
	"Kafka - Usage - UseCase - Streaming - LogProcessing ": "Log Processing and Analysis: Efficiently handles massive volumes of log data for analysis and insight generation.",
	"Kafka - Usage - UseCase - Streaming - DataStreaming ": "Data Streaming for Recommendations: Powers real-time data streaming to deliver personalized recommendations.",
	"Kafka - Usage - UseCase - Streaming - SystemMonitoring ": "System Monitoring and Alerting: Facilitates real-time monitoring and alerting systems for timely responses to system metrics.",
	"Kafka - Usage - UseCase - Streaming - ChangeDataCapture ": "Change Data Capture (CDC): Captures and processes database changes to keep data in sync across systems.",
	"Kafka - Usage - UseCase - Streaming - SystemMigration ": "System Migration: Supports the seamless migration of systems by ensuring data consistency and availability.",
	"Kafka - Usage - UseCase - Streaming - MonitorLag ": "Use Kafka Manager or Prometheus to track consumer lag.",
	"Kafka - Usage - UseCase - Streaming - StreamProcessing ": "Use Kafka Streams or Flink for real-time processing.",
	"Kafka - Usage - UseCase - Storage": "Storing data across multiple servers.",
	"Kafka - Components - Producer": "Producers in Kafka create new messages, batch them, and send them to a Kafka topic.",
	"Kafka - Components - Producer - Tuning": "Set acks=all, tweak linger.ms & batch.size for efficiency.",
	"Kafka - Components - Producer - Idempotent": "Enable enable.idempotence=true for exactly-once delivery.",
	"Kafka - Components - Producer - Ackhnowledgements": "",
	"Kafka - Components - Producer - Ackhnowledgements - acks=0": "Fire-and-forget (no guarantee).",
	"Kafka - Components - Producer - Ackhnowledgements - acks=1": "Leader acknowledgment.",
	"Kafka - Components - Producer - Ackhnowledgements - acks=11": "Leader + replicas acknowledgment (strongest guarantee).",
	"Kafka - Components - Producer - KafkaConnect": "Kafka Connect allows you to continuously ingest data from external systems into Kafka, and vice versa.",
  "Kafka - Components - Broker": "Store and distribute data across servers.",
  "Kafka - Components - Broker - Message": "Message is the basic unit of data in Kafka. It consists of headers, key, and value.",
  "Kafka - Components - Broker - Message - Features - Durability": ".",
  "Kafka - Components - Broker - Message - Features - FaultTolerance": ".",
  "Kafka - Components - Broker - Message - Storage - Log": "Kafka stores messages in a log-structured format on disk.",
  "Kafka - Components - Broker - Message - Storage - Log - Segments": "Partitions are stored as a series of files called log segments.",
  "Kafka - Components - Broker - Message - Storage - Log - Segments - Immutable": "These segments are immutable—once written, they don’t change—and new messages are appended to the active segment.",
  "Kafka - Components - Broker - Message - Storage - Log - Segments - Size": "When a segment reaches a size or time limit (configurable), it’s closed, and a new segment is created.",
  "Kafka - Components - Broker - Message - Storage - Log - Segments - Retain": "Older segments are retained based on retention policies (e.g., time or size limits) and eventually deleted when no longer needed.",
  "Kafka - Components - Broker - Message - Storage - Log - Offset": "The unique ID of each message. Since offset 100 could reside in any segment of a partition, the broker must efficiently locate it without scanning every message from the beginning.",
  "Kafka - Components - Broker - Message - Storage - Log - Offset - Index": "To enable fast message lookups, Kafka maintains an offset index for each partition.",
  "Kafka - Components - Broker - Message - Storage - Log - Offset - Index - Structure - Location": "Offsets (e.g., 100, 110, 120, etc.) → Physical location in the log (i.e., which segment file and position within that file the message is stored).",
  "Kafka - Components - Broker - Message - Storage - Log - Offset - Index - Structure - Entries": "The index doesn’t map every single offset (to avoid inefficiency); instead, it records entries periodically (e.g., every 4 KB of data, configurable via index.interval.bytes).",
  "Kafka - Components - Broker - Message - Storage - Log - Offset - Locating": "𝙒𝙝𝙚𝙣 𝙖 𝙘𝙤𝙣𝙨𝙪𝙢𝙚𝙧 𝙧𝙚𝙦𝙪𝙚𝙨𝙩𝙨 𝙤𝙛𝙛𝙨𝙚𝙩 100, 𝙩𝙝𝙚 𝙗𝙧𝙤𝙠𝙚𝙧, • Checks the offset index for the closest indexed offset ≤ 100. • Jumps to the corresponding segment file and position. • Scans sequentially from there to reach offset 100.",
  "Kafka - Components - Broker - Message - Storage - Log - Compaction": "",
  "Kafka - Components - Broker - Message - Ordering": ".",
  "Kafka - Components - Broker - Message - Delivery": ".",
  "Kafka - Components - Broker - Message - Delivery - Semantics - at-least-once": ".",
  "Kafka - Components - Broker - Message - Delivery - Semantics - exactly-once": ".",
  "Kafka - Components - Broker - Message - Delivery - Semantics - at-most-once": ".",
  "Kafka - Components - Broker - Queues": "Consumer Groups allow Kafka to behave like a Queue, since each consumer instance in a group processes data from a non-overlapping set of partitions (within a Kafka topic).",
  "Kafka - Components - Broker - Topics": "Messages are organized into topics, which are divided into partitions for scalability. Every message goes to a particular Topic. Topics have multiple partitions.",
  "Kafka - Components - Broker - Topics - RetentionPolicy": "Adjust log.retention.hours & segment.bytes wisely.",
	"Kafka - Components - Broker - Topics - Partitions": "For fault tolerance and scalability, a Kafka topic is further divided into units called partitions.",
	"Kafka - Components - Broker - Topics - Partitions - Storage": "Each partition represents a portion of the topic’s data and is stored in a single log file.",
	"Kafka - Components - Broker - Topics - Partitions - Scalability": "Partitioning enables parallel message distribution among several brokers in the cluster, facilitating scalability for consumers and producer",
	"Kafka - Components - Broker - Topics - Partitions - Strategies": "",
	"Kafka - Components - Broker - Cluster": "A Kafka cluster consists of several brokers where each partition is replicated across multiple brokers to provide high availability and redundancy.",
	"Kafka - Components - Broker - Cluster - Node": ".",
	"Kafka - Components - Broker - Cluster - InSyncReplicas": "A set of replicas (leader + followers) that are fully caught up with the leader. Only ISR members can become leaders.",
	"Kafka - Components - Broker - Cluster - ZooKeeper": "In Apache Kafka, ZooKeeper, a distributed coordination service, manages cluster metadata, broker registration, leader election, and topic/partition information, ensuring the reliability and coordination of the Kafka cluster. ",
	"Kafka - Components - Consumer": "Kafka consumers work together as a consumer group to read messages from the broker. ",
	"Kafka - Components - Consumer - Balanced": "Ensure even consumer distribution across partitions.",
	"Kafka - Components - Consumer - Group": "A set of consumers that work together.",
	"Kafka - Components - Consumer - Tuning": ".",
	"Kafka - Components - KafkaStreams": "Kafka Streams is a lightweight library developed by Apache Kafka that enables real-time stream processing of data stored in Kafka topics.",

	"NATS": "SECTION",
  "NATS - Features - MesageModel": "Subject-based (pub-sub)",
  "NATS - Features - Persistence": "Optionally, memory first with persistance",
  "NATS - Features - Performance": "Extremely fast and light weight",

	"Bufstream": "SECTION",
  "Bufstream - Features - MesageModel": "",
	"Bufstream ": "https://www.linkedin.com/posts/nikkisiapno_kafka-vs-bufstream-whats-the-difference-activity-7307343842318462976-Agz1/",

	"Brokers - Mosquito": "",
	"Brokers - Pushy": "",
	"Brokers - VerneMQ": "",
	"Brokers - IBM Bluemix": "",
	"Brokers - IBM IoT MessageSight": "",
	"Brokers - IBM WebSphere MQ Telemetry": "",
	"Brokers - HiveMQ": "",
	"Brokers - Azure IoT": "",
	"Brokers - ThingMQ": "",
	"Brokers - ThingStudio": "",
	"Brokers - cloudMQTT": "",
	"Brokers - Heroku CloudMQTT": "",
	"Brokers - MachineHead": "",
	"Brokers - Mercury": "This is a message broker that enables some common messaging patterns over WebSockets.",
	"Brokers - JMS PTP": "",
	"Brokers - ZeroMQ": "",
	"Brokers - AWS SQS": "",
	"Brokers - AWS SNS": "",
	"Brokers - AWS Kinesis": "",
	"Brokers - AWS AmazonMQ": "",
	"Brokers - Microsoft Azure Event Hubs": "",
	"Brokers - Google pub/sub": "",
	"Brokers - Spark Streams": ""

}