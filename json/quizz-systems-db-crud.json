{

	"Consuming": "SECTION",
	"Consuming - Pagination": "",
	"Consuming - Pagination - OffsetBased": "This technique uses an offset and a limit parameter to define the starting point and the number of records to return. Example: GET /orders?offset=0&limit=3 ",
	"Consuming - Pagination - CursorBased": "This technique uses a cursor (a unique identifier) to mark the position in the dataset. Typically, the cursor is an encoded string that points to a specific record. Example: GET /orders?cursor=xxx ",
	"Consuming - Pagination - PageBased": "This technique specifies the page number and the size of each page. Example: GET /items?page=2&size=3 ",
	"Consuming - Pagination - KeysetBased": "This technique uses a key to filter the dataset, often the primary key or another indexed column. Example: GET /items?after_id=102&limit=3 ",
	"Consuming - Pagination - TimeBased": "This technique uses a timestamp or date to paginate through records. Example: GET /items?start_time=xxx&end_time=yyy ",
	"Consuming - Pagination - Hybrid": "This technique combines multiple pagination techniques to leverage their strengths. Example: Combining cursor and time-based pagination for efficient scrolling through time-ordered records. ",
	"Consuming - Caching - purpose": "Storing frequently used data in a temporary location that is faster to access than the original source.",
	"Consuming - Caching - Strategies - Write - WriteThrough": "By simultaneously writing to the cache and the database.",
	"Consuming - Caching - Strategies - Write - WriteBack": "Data is written to the cache first and then to the database 'asynchronously.",
	"Consuming - Caching - Strategies - Write - WriteAround": "Bypasses the cache for writes, writing directly to the database. The cache is updated only on subsequent reads.",
	"Consuming - Caching - Strategies - Read - ClientSideCaching": "Data is cached on the client's device.",
	"Consuming - Caching - Strategies - Read - CacheAside": "This strategy grants applications explicit control over the caching process.",
	"Consuming - Caching - Strategies - Read - ReadThrough": "The cache acts as the primary data source. If data isn't in the cache, it's retrieved from the database and stored in the cache.",
	"Consuming - Caching - Strategies - Read - RefreshAhead": "The cache proactively refreshes data before it expires.",
	"Consuming - Caching - Strategies - Eviction - LRU": "𝐋𝐞𝐚𝐬𝐭 𝐑𝐞𝐜𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐋𝐑𝐔) - Removes the least recently accessed item first. - Works well when older data is less likely to be used again. - Example: Browsers use LRU to discard old pages from memory when opening new tabs.",
	"Consuming - Caching - Strategies - Eviction - LFU": "𝐋𝐞𝐚𝐬𝐭 𝐅𝐫𝐞𝐪𝐮𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐋𝐅𝐔) - Evicts the least accessed items over time. - Prioritizes keeping frequently used items in cache. - Example: CDNs use LFU to keep trending videos in cache while removing rarely watched ones.",
	"Consuming - Caching - Strategies - Eviction - MRU": "𝐌𝐨𝐬𝐭 𝐑𝐞𝐜𝐞𝐧𝐭𝐥𝐲 𝐔𝐬𝐞𝐝 (𝐌𝐑𝐔) - Opposite of LRU – evicts the most recently accessed item first. - Useful when recent data becomes obsolete quickly. - Example: In a music streaming app, the last played song is less likely to be played again, making MRU a better choice.",
	"Consuming - Caching - Strategies - Eviction - TTL": "𝐓𝐢𝐦𝐞 𝐭𝐨 𝐋𝐢𝐯𝐞 (𝐓𝐓𝐋) - Items are evicted after a set time limit (expiry time). - Prevents stale data, useful in distributed systems. - Example: DNS records have TTL values, so if an IP address changes, clients don’t hold onto outdated mappings.",
	"Consuming - Caching - Strategies - Eviction - FIFO": "𝐅𝐢𝐫𝐬𝐭 𝐈𝐧, 𝐅𝐢𝐫𝐬𝐭 𝐎𝐮𝐭 (𝐅𝐈𝐅𝐎) - Evicts the oldest stored item first, regardless of usage. - Simple to implement but may remove still-relevant data. - Example: Simple queue-based caching systems.",
	"Consuming - Caching - Strategies - Eviction - RR": "𝐑𝐚𝐧𝐝𝐨𝐦 𝐑𝐞𝐩𝐥𝐚𝐜𝐞𝐦𝐞𝐧𝐭 (𝐑𝐑) - Randomly evicts an item when the cache is full. - Low overhead, but less predictable performance. - Example: Some network routers use RR for managing limited memory, avoiding complex eviction policies.",
	"Consuming - Caching - Strategies - Eviction - TT": "𝐓𝐰𝐨-𝐓𝐢𝐞𝐫𝐞𝐝 𝐂𝐚𝐜𝐡𝐢𝐧𝐠 - Uses a fast in-memory cache (e.g., Redis) & a slower persistent cache (e.g., disk-based). - Optimizes speed & storage by balancing hot and cold data. - Example: A CDN stores frequently accessed content in RAM (L1 cache) while keeping less-accessed content on disk (L2 cache) to balance performance and cost.",
	"Consuming - Caching - Tools - VarnishCache": ".",

	"Querying": "SECTION",
	"Querying - Transactions": "https://medium.com/@patrickkoss/interview-so-how-do-you-do-a-transaction-with-nosql-databases-c3d80bc7d314",
	"Querying - Transactions - Principles - ACID": "ACID: Atomicity, Consistency, Isolation, Durability",
	"Querying - Transactions - Principles - ACID - Atomicity": "The DB must treat each transaction as all or nothing, if any part fails, the whole transaction must be rolled back as if it never happened.",
	"Querying - Transactions - Principles - ACID - Atomicity - Orthogonality": "Atomicity does not behave completely orthogonally with regard to the other ACID properties of transactions. For example, isolation relies on atomicity to roll back the enclosing transaction in the event of an isolation violation such as a deadlock; consistency also relies on atomicity to roll back the enclosing transaction in the event of a consistency violation by an illegal transaction. As a result of this, a failure to detect a violation and roll back the enclosing transaction may cause an isolation or consistency failure.",
	"Querying - Transactions - Principles - ACID - Consistency": "A catch-all term that means all the rules defined in the database must be followed when committing a transaction. The end state, at the end of the transaction, must be valid. Rules here can mean data constraints, cascades, triggers, etc. Consistency relies on atomicity, in that if there is a violation, we rely on the systems ability to roll back changes.",
	"Querying - Transactions - Principles - ACID - Isolation": "The isolation determines the visibility of changes made by one transaction for another transactions. Ideally, the result of 2 concurrent operations should be the same as if they occurred sequentially.",
	"Querying - Transactions - Principles - ACID - Isolation ": "https://medium.com/@patrickkoss/interview-so-how-do-you-do-a-transaction-with-nosql-databases-c3d80bc7d314",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions": "A transaction is an unit of work that helps to respect ACID principle. This unit of work can only be executed entirely. It means that database will see zero (if transaction fails, operation called rollback) or all operations placed inside transaction (if transaction succeeds, operation called commit).",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions - levels": "They're 4 mains transaction's isolation levels: read uncommited, read commited, repeatable read, serializable",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions - levels - ReadUncommited": "Read Uncommitted is the lowest isolation level. In this level, one transaction may read not yet commited changes made by other transaction, thereby allowing dirty reads. In this level, transactions are not isolated from each other.",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions - levels - ReadCommitted": "This isolation level guarantees that any data read is committed at the moment it is read. Thus it does not allows dirty read. The transaction hold a read or write lock on the current row, and thus prevent other rows from reading, updating or deleting it.",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions - levels - RepeatableRead": "This is the most restrictive isolation level. The transaction holds read locks on all rows it references and write locks on all rows it inserts, updates, or deletes. Since other transaction cannot read, update or delete these rows, consequently it avoids non repeatable read.",
	"Querying - Transactions - Principles - ACID - Isolation - Transactions - levels - Serializable": "This is the Highest isolation level. A serializable execution is defined to be an execution of the operations of concurrently executing SQL-transactions that produces the same effect as some serial execution of those same SQL-transactions. A serial execution is one in which each SQL-transaction executes to completion before the next SQL-transaction begins.",
	"Querying - Transactions - Principles - ACID - Isolation - Reads": "Transaction isolation levels can lead to incoscient data reads. We can distinguish 3 types of these reads: Dirty, Phantom and NonRepeatable",
	"Querying - Transactions - Principles - ACID - Isolation - Reads - States - DirtyRead ": " a transaction reads a data that has not yet been commited.",
	"Querying - Transactions - Principles - ACID - Isolation - Reads - States - PhantomRead": "Phantom Read occurs when two same queries are executed, but the rows retrieved by the two, are different.",
	"Querying - Transactions - Principles - ACID - Isolation - Reads - States - NonRepeatableRead": "a transaction reads same row twice, and get a different value each time. For example, suppose transaction T1 reads a data. Due to concurrency, another transaction T2 updates the same data and commit, Now if transaction T1 rereads the same data, it will retrieve a different value.",
	"Querying - Transactions - Principles - ACID - Durability": "Transactions that have committed must survive permanently, even if the system crashes. This is usually assured by writing transactions into a log before acknowledging the commit. The log is on non-volatile storage and can be used to recreate the system state prior to failure (often automatically).",
	"Querying - Transactions - Principles - BASE": ".",
	"Querying - Transactions - Principles - BASE - ConsistencyModel": "BASE properties are looser than ACID. A BASE datastore values availability and scale, instead of guaranteed consistency. Used by NoSQL stores, including column family, key-value and document stores.",
	"Querying - Transactions - Principles - BASE - BasicAvailability": "The database appears to work most of the time.",
	"Querying - Transactions - Principles - BASE - SoftState": "Stores do not have to be write-consistent, nor do different replicas have to be mutually consistent all the time.",
	"Querying - Transactions - Principles - BASE - EventualConsistency": "Stores exhibit consistency at some later point (e.g., lazily at read time).",
	"Querying - Transactions - Principles - NewSQL": "https://levelup.gitconnected.com/newsql-databases-the-best-of-both-worlds-8727411a49ec",
	"Querying - Transactions - JTA": "Java Transaction API",
	"Querying - Transactions - XA": "XA is a two-phase commit protocol that is natively supported by many databases and transaction monitors.",
	"Querying - Transactions - TransactionalMonitor": "",

	"DataOptimization": "SECTION",
	"DataOptimization - Indexing ": "A special data structure related to a database table and used for storing its important parts and enabling faster data search and retrieval. Indexes are especially efficient for large databases, where they significantly enhance query performance.",
	"DataOptimization - Indexing - Unique": "Unique index – doesn't allow duplicates in a table column and hence helps maintain data integrity.",
	"DataOptimization - Indexing - Clustered": "Clustered index – defines the physical order of records of a database table and performs data searching based on the key values. A table can have only one clustered index.",
	"DataOptimization - Indexing - Non-clustered": "Non-clustered index – keeps the order of the table records that don't match the physical order of the actual data on the disk. It means that the data is stored in one place and a non-clustered index – in another one. A table can have multiple non-clustered indexes.",
	"DataOptimization - Indexing - B-Tree": "B-trees, short for balanced trees, are the most common type of database index. A B-tree index is an ordered list of values divided into ranges. By associating a key with a row or range of rows, B-trees provide excellent retrieval performance for a wide range of queries, including exact match and range searches.",
	"DataOptimization - Indexing - Bitmap": "In a bitmap index, the database stores a bitmap for each index key. In a conventional B-tree index, one index entry points to a single row. In a bitmap index, each index key stores pointers to multiple rows. Bitmap indexes are primarily designed for data warehousing or environments in which queries reference many columns in an ad hoc fashion.",
	"DataOptimization - Managing - Archiving": "We implement archiving strategies to keep active data lean and fast, moving older data to separate storage.",
	"DataOptimization - Scaling - Replication": "We design robust replication setups to distribute read loads and enhance availability. It's like cloning your database for peak performance.",
	"DataOptimization - Scaling - Sharding": "For massive datasets, we implement sharding to divide and conquer, distributing the load across multiple servers.",
	"DataOptimization - Scaling - Partitioning": "We use partitioning to break large tables into manageable chunks, improving query performance and simplifying maintenance.",
	"DataOptimization - Design - Normalization": "Optimize for CRUDing (OLTP).",
	"DataOptimization - Design - Normalization - Features - Pros": "Better data integrity, less redundancy, easier maintenance.",
	"DataOptimization - Design - Normalization - Features - Cons": "Can slow down read operations due to multiple table joins.",
	"DataOptimization - Design - Normalization - Anomaly - Insert": "An insert anomaly occurs when you cannot add new data to the database without also adding irrelevant or redundant data. This typically happens in a poorly normalized table where some columns are dependent on others.",
	"DataOptimization - Design - Normalization - Anomaly - Delete": "A delete anomaly occurs when the deletion of a record results in the unintended loss of additional data that was associated with the record. This usually happens in a table where data is not properly normalized, causing related information to be removed inadvertently.",
	"DataOptimization - Design - Normalization - Anomaly - Update": "An update anomaly occurs when changes to data require multiple rows to be updated, leading to potential inconsistencies. This usually happens when the same piece of information is duplicated across multiple rows.",
	"DataOptimization - Design - Normalization - Forms - 1NF": "First Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Rule": "Ensure each column in a table contains unique and indivisible values.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Reason": "This eliminates repeating groups and ensures that the data in each column is atomic, meaning that it cannot be divided further. It helps in simplifying the data structure and avoids the pitfalls of storing multiple values in a single column.",
	"DataOptimization - Design - Normalization - Forms - 1NF - Impact": "While 1NF ensures a basic level of data integrity by making data retrieval straightforward, it is not sufficient to handle more complex data dependencies and anomalies. Without further normalization, the database might still suffer from redundancy and update anomalies.",
	"DataOptimization - Design - Normalization - Forms - 2NF": "Second Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Rule": "Ensure all non-key columns are fully dependent on the entire primary key.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Reason": "This addresses the issue of partial dependency, where non-key attributes are dependent on part of a composite primary key. It ensures that each non-key attribute is only dependent on the primary key, thus eliminating redundancy for non-key attributes.",
	"DataOptimization - Design - Normalization - Forms - 2NF - Impact": "By achieving 2NF, the database reduces redundancy and ensures that updates to the data are consistent. However, it might still face issues related to transitive dependencies, which are addressed in 3NF.",
	"DataOptimization - Design - Normalization - Forms - 3NF": "Third Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 3NF - Rule": "Remove any columns that depend on non-key columns (transitive dependency).",
	"DataOptimization - Design - Normalization - Forms - 3NF - Reason": "This ensures that all attributes are only dependent on the primary key and not on other non-key attributes. It eliminates the potential for anomalies that can occur when non-key attributes are dependent on other non-key attributes.",
	"DataOptimization - Design - Normalization - Forms - 3NF - Impact": "3NF improves data integrity by ensuring that changes in non-key attributes do not lead to inconsistencies. It also enhances query performance by reducing the number of redundant data and simplifying the schema.",
	"DataOptimization - Design - Normalization - Forms - BCNF": "Boyce-Codd Normal Form (BCNF).",
	"DataOptimization - Design - Normalization - Forms - BCNF - Rule": "A stricter version of 3NF where every determinant must be a candidate key.",
	"DataOptimization - Design - Normalization - Forms - BCNF - Reason": "BCNF addresses situations where 3NF does not fully eliminate anomalies, particularly in cases where a table has multiple overlapping candidate keys.",
	"DataOptimization - Design - Normalization - Forms - BCNF - Impact": "Achieving BCNF further strengthens data integrity by ensuring that every determinant is a candidate key, thus removing any remaining anomalies and ensuring a more robust schema.",
	"DataOptimization - Design - Normalization - Forms - 4NF": "Fourth Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Rule": "Address even more complex dependencies, such as multi-valued dependencies.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Reason": "These forms are designed to handle more complex scenarios that are not addressed by the earlier normal forms. For example, 4NF deals with multi-valued dependencies, ensuring that one attribute cannot have multiple independent multi-valued facts associated with it.",
	"DataOptimization - Design - Normalization - Forms - 4NF - Impact": "Higher normal forms provide even more stringent controls over data integrity and further reduce redundancy, though they are less commonly applied in everyday database design.",
	"DataOptimization - Design - Normalization - Forms - 5NF": "Fifth Normal Form.",
	"DataOptimization - Design - Normalization - Forms - 5NF - Rule": "A relation is said to be in 5FN if it is in 4FN and it cannot be further decomposed into smaller tables.",
	"DataOptimization - Design - Normalization - Forms - 5NF - Reason": ".",
	"DataOptimization - Design - Normalization - Forms - 5NF - Impact": ".",
	"DataOptimization - Design - Denormalization": "Optimize for querying (OLAP).",
	"DataOptimization - Design - Denormalization - Features - Pros": "Faster read operations, simpler queries.",
	"DataOptimization - Design - Denormalization - Features - Cons": "More redundancy, higher risk of data inconsistencies, harder maintenance.",
	"DataOptimization - Design - Denormalization - Methods - Redundant": "Adding Redundant Data: Include extra copies of data to avoid complicated lookups..",
	"DataOptimization - Design - Denormalization - Methods - Aggregates": "Creating Aggregates: Store pre-calculated summary data to speed up queries.",
	"DataOptimization - Design - Denormalization - Methods - Combining": "Combining Tables: Merge related tables to simplify queries..",

	"DataModel": "SECTION",
	"DataModel - DataStructures - View": ".",
	"DataModel - DataStructures - MaterializedView": "We pre-compute and store complex query results, reducing processing time and resource usage. Think of it as caching for your most demanding queries.",
	"DataModel - DataStructures - all": "https://medium.com/@ashishps/how-i-mastered-data-structures-and-algorithms-eb8c5273c56d",
	"DataModel - DataStructures - DataTypes": ".",
	"DataModel - Types - RBDMS": "In a 𝗿𝗲𝗹𝗮𝘁𝗶𝗼𝗻𝗮𝗹 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲, data is organized in rows and columns where a row represents a record and its data fields are stored in columns. They are ideal for when ACID compliance is required, and a predefined schema can be created.",
	"DataModel - Types - RBDMS - MySQL": "",
	"DataModel - Types - RBDMS - PostgresSQL": "",
	"DataModel - Types - Column": "With 𝗰𝗼𝗹𝘂𝗺𝗻𝗮𝗿 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀, records are stored as columns rather than rows. This makes them very performant for analytical purposes where complex queries are run across large datasets; especially those that contain aggregate functions.",
	"DataModel - Types - Column - BigQuery": "",
	"DataModel - Types - Column - Cassandra": "",
	"DataModel - Types - Document": "In a 𝗱𝗼𝗰𝘂𝗺𝗲𝗻𝘁 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲, data is stored in a semi-structured format such as JSON. They offer a flexible and schema-less approach which makes them a great choice for data with complex or continually changing structures.",
	"DataModel - Types - Document - MongoDB": "",
	"DataModel - Types - Document - CouchDB": "",
	"DataModel - Types - Graph": "𝗚𝗿𝗮𝗽𝗵 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 are optimized for storing and querying highly connected data. Records are represented as nodes and relationships as edges. Under the hood, they use graph theory to traverse relationships between nodes to power performant queries.",
	"DataModel - Types - Graph - Neo4J": "",
	"DataModel - Types - Graph - Neptune": "",
	"DataModel - Types - KeyValueStores": "𝗞𝗲𝘆-𝘃𝗮𝗹𝘂𝗲 𝘀𝘁𝗼𝗿𝗲𝘀 are a simple form of storage where values are inserted, updated, and retrieved using a unique key. They are more commonly used for small datasets and often temporary purposes such as caching or session management.",
	"DataModel - Types - KeyValueStores - Redis": "",
	"DataModel - Types - KeyValueStores - DynamoDB": "",
	"DataModel - Types - Vector": "𝗩𝗲𝗰𝘁𝗼𝗿 𝗱𝗮𝘁𝗮𝗯𝗮𝘀𝗲𝘀 store data as vectors, enabling fast similarity searches for high-dimensional data. They are ideal for AI & ML applications, such as recommendations and searching unstructured data like images, text, or audio.",
	"DataModel - Types - Vector - Milvus": "",
	"DataModel - Types - Vector - Pinecone": "",
	"DataModel - Types - TimeSeriesDB": "",
	"DataModel - Types - TimeSeriesDB - InfluxDB": "for time-stamped data like logs and metrics",
	"DataModel - Types - TimeSeriesDB - TimescaleDB": "extension for PostgreSQL.",
	"DataModel - Types - InMemoryDB": "",
	"DataModel - Types - InMemoryDB - Redis": "for caching and real-time data processing. https://www.linkedin.com/posts/alexandre-zajac_softwareengineering-systemdesign-programming-activity-7315273057915289600-XxIa/?utm_source=share&utm_medium=member_android&rcm=ACoAAABC9LwBrHjPW40o31rZRtAXH6eii8ctLzQ",
	"DataModel - Types - InMemoryDB - Memcache": "for high-performance distributed memory object caching.",
	"DataModel - Types - NewSQL": "",
	"DataModel - Types - NewSQL - GoogleSpanner": "for combining NoSQL scalability with traditional RDBMS features",
	"DataModel - Types - NewSQL - CockroachDB": "for distributed SQL.",
	"DataModel - ID - UUID": "A UUID (Universally Unique Identifier) is a 128-bit number used to uniquely identify objects or records in computer systems. There are multiple versions of UUIDs.",
	"DataModel - ID - UUID - unique": "UUIDs are not guaranteed to be unique, instead they are given a certain (typically very low) probability of collision. The probability of generating two UUIDs (especially v4 or v7) that collide is very low, but not zero.",
	"DataModel - ID - UUIDv1 - Time-based": "A combination of the current timestamp and the MAC address of the machine, unique but may expose hardware information.",
	"DataModel - ID - UUIDv2 - DCE Security": "Similar to Version 1 but includes POSIX UID/GID information, for applications requiring user or group identification.",
	"DataModel - ID - UUIDv3 - Name-based, MD5": "Hashes a namespace identifier and a name using the MD5 algorithm, producing consistent UUIDs for the same input data.",
	"DataModel - ID - UUIDv4 - Random": "Uses random numbers, offering simplicity and a low probability of duplication. e942bbe9-afdc-4c62-a438-4efee77954b3 You can tell it’s UUIDv4 because the digit ‘4’ appears in the 13th position. This is a key identifier for the version of the UUID.",
	"DataModel - ID - UUIDv5 - Name-based, SHA-1": "Similar to Version 3 but uses the SHA-1 hashing algorithm, providing a more secure hash function for generating UUIDs from names.",
	"DataModel - ID - UUIDv6 - Ordered Time-based": "A reordering of Version 1 UUIDs to improve database indexing by placing the timestamp in the most significant bits, facilitating chronological ordering. May be suitable for some use cases of database keys needing ordering.",
	"DataModel - ID - UUIDv7 - Unix Epoch Timestamp": "Encodes a Unix timestamp with millisecond precision in the most significant 48 bits, followed by random data, ensuring uniqueness and time-ordering. 01922e13-43f2-79ef-ab97-ca7a7d021d34 UUIDv7 is a time-based version of UUID, meaning that the identifiers are generated in an increasing order.",
	"DataModel - ID - UUIDv7 - ordered": "Because the UUIDs are time-based, they are ordered, which leads to better indexing performance compared to the randomness of UUIDv4.",
	"DataModel - ID - UUIDv8 - Custom": "Reserved for custom implementations, allowing for the inclusion of application-specific data within the UUID structure.",
	"DataModel - ID - ULID": "Universally Unique Lexicographically Sortable Identifier. ULID is a UUID alternative that is also globally unique, but with the added benefit of being lexicographically sortable.",
	"DataModel - ID - Auto-IncrementingIDs": ".",
	"DataModel - ID - SnowflakeID": "Twitter. A distributed ID generation algorithm that generates 64-bit unique IDs using a combination of a timestamp, machine ID, and sequence number.",
	"DataModel - ID - KSUID": "KSUID (K-Sortable Unique Identifier). KSUIDs are a variation of UUIDs that include a timestamp, making them sortable by creation time. A 27-character string consisting of a timestamp and randomly generated bits, ensuring k-sortability.",
	"DataModel - ID - NanoID": "NanoID is a small, fast, and secure alternative to UUID, designed to be URL-friendly and customizable in terms of size. A short, random, URL-friendly string with customizable length and alphabet.",
	"DataModel - ID - RandomHash-BasedID": "Random Hash-Based ID (SHA-256 or MD5 Hashing). Randomly generated strings using hash functions like SHA-256 or MD5 to create unique identifiers. A fixed-length 32- or 64-character string generated by hashing data (like a combination of timestamp and user data).",
	"DataModel - ID - ObjectID": "ObjectID (MongoDB ObjectID). MongoDB’s ObjectID (BSON binary JSON) is a 12-byte unique identifier that includes a timestamp, machine identifier, process identifier, and a counter.",
	"DataModel - ID - CUID2": "CUID2 (Collision-Resistant Unique Identifier). Cuid2 is designed to minimize the likelihood of collision in distributed systems, providing a URL-safe, human-readable, and collision-resistant ID.",
	"DataModel - ID - FlakeID": "Includes a timestamp, machine identifier, and sequence number. If you require unique, time-ordered identifiers for easier sorting and debugging. Example: 304857642123456",
	"DataModel - ID - Time-basedIDs": "Often a Unix timestamp concatenated with random bits or other unique data. Where chronological order is important, such as logging or event tracking.",
	"DataModel - ID - SequentialGUIDs": "A GUID optimized for indexing, often beginning with a time-ordered segment. Where GUIDs are necessary, but ordered indexing is needed for better database performance. Example: 6E4F6A80-4F64-11EE-B4FA-0242AC120002",
	"DataModel - ID - ShortID": "Generates a compact, unique alphanumeric string, often used in URLs. URL shortening or user-friendly identifiers in public URLs. Example: 2K5czP8",
	"DataModel - ID - ZUID": "ZUID (Zero-width Unique Identifier): Involves zero-width characters (e.g., zero-width spaces) that are invisible but can be parsed for uniqueness. If you need invisible identifiers for tracking or metadata without impacting visual layout. Example: Internally may look like \u200B\u200C\u200D"

}