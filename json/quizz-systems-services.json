{

	"Apps": "SECTION",
	"Apps - ServiceDiscovery": "",
	"Apps - ServiceDiscovery - ClientSide": "When making a request to a service, the client obtains the location of a service instance by querying a Service Registry, which knows the locations of all service instances.",
	"Apps - ServiceDiscovery - ServerSide": "When making a request to a service, the client makes a request via a router (a.k.a load balancer) that runs at a well known location. The router queries a service registry, which might be built into the router, and forwards the request to an available service instance.",
	"Apps - ServiceDiscovery - Registry": "Implement a service registry, which is a database of services, their instances and their locations. Service instances are registered with the service registry on startup and deregistered on shutdown. Client of the service and/or routers query the service registry to find the available instances of a service. A service registry might invoke a service instance’s health check API to verify that it is able to handle requests",
	"Apps - ServiceDiscovery - Registry - SelfRegistration": "A service instance is responsible for registering itself with the service registry. On startup the service instance registers itself (host and IP address) with the service registry and makes itself available for discovery. The client must typically periodically renew its registration so that the registry knows it is still alive. On shutdown, the service instance unregisters itself from the service registry.",
	"Apps - ServiceDiscovery - Registry - 3rdPartyRegistration": "A 3rd party registrar is responsible for registering and unregistering a service instance with the service registry. When the service instance starts up, the registrar registers the service instance with the service registry. When the service instance shuts downs, the registrar unregisters the service instance from the service registry.",
	"Apps - ServiceDiscovery - Registry - Product - Eureka": "",
	"Apps - ServiceDiscovery - Registry - Product - Consul": "",
	"Apps - ServiceDiscovery - Registry - Product - Zookeeper": "",
	"Apps - ServiceDiscovery - Standards - BDXL": "Business Document Metadata Service Location (BDXL)",
	"Apps - ServiceDiscovery - Standards - SMP": "Service Metadata Publishing (SMP)",

	"Networking": "SECTION",
	"Networking - Protocols - HTTP": "",
	"Networking - Protocols - HTTP - HTTP": "",
	"Networking - Protocols - HTTP - HTTP - keep-alive": "Keep Connections Alive: Enable HTTP keep-alive to avoid re-establishing connections for every request.",
	"Networking - Protocols - HTTP - HTTP2": "HTTP/2 began as the SPDY protocol, developed primarily at Google",
	"Networking - Protocols - HTTP - HTTP2 - Goal": "Reducing server latency",
	"Networking - Protocols - HTTP - HTTP2 - Features - Binary": "binary framing layer. As opposed to HTTP/1.1, which keeps all requests and responses in plain text format, HTTP/2 uses the binary framing layer to encapsulate all messages in binary format, while still maintaining HTTP semantics, such as verbs, methods, and headers.",
	"Networking - Protocols - HTTP - HTTP2 - Features - Headers - compression": "Small files load more quickly than large ones. To speed up web performance, both HTTP/1.1 and HTTP/2 compress HTTP messages to make them smaller. However, HTTP/2 uses a more advanced compression method called HPACK that eliminates redundant information in HTTP header packets. This eliminates a few bytes from every HTTP packet. Given the volume of HTTP packets involved in loading even a single webpage, those bytes add up quickly, resulting in faster loading.",
	"Networking - Protocols - HTTP - HTTP2 - Features - Prioritization": "Stream prioritization not only solves the possible issue of requests competing for the same resource, but also allows developers to customize the relative weight of requests to better optimize application performance.",
	"Networking - Protocols - HTTP - HTTP2 - Features - Multiplexing": "HTTP/1.1 loads resources one after the other, so if one resource cannot be loaded, it blocks all the other resources behind it. In contrast, HTTP/2 is able to use a single TCP connection to send multiple streams of data at once so that no one resource blocks any other resource. HTTP/2 does this by splitting data into binary-code messages and numbering these messages so that the client knows which stream each binary message belongs to.",
	"Networking - Protocols - HTTP - HTTP2 - Features - Push": "Server push: Typically, a server only serves content to a client device if the client asks for it. However, this approach is not always practical for modern webpages, which often involve several dozen separate resources that the client must request. HTTP/2 solves this problem by allowing a server to 'push' content to a client before the client asks for it. The server also sends a message letting the client know what pushed content to expect – like if Bob had sent Alice a Table of Contents of his novel before sending the whole thing.",
	"Networking - Protocols - HTTP - HTTP3 - Transport": "Both HTTP/1.1 and HTTP/2 use TCP as their transport, whereas HTTP/3 is based on Google’s QUIC – a transport layer network protocol that implements user space congestion control over UDP (User Datagram Protocol).",
	"Networking - Firewall": "",
	"Networking - RateLimiting": "Rate limiting is a crucial technique in controlling the amount of traffic a server receives within a specified time frame. It's used to prevent overuse of resources, improve server reliability, and ensure fair usage among users. Rate limiting is common in API management to prevent abuse and to manage traffic effectively.",
	"Networking - RateLimiting - Algorithms": "In implementing rate limiting, it's crucial to choose an algorithm that aligns with the system's needs, balancing between fairness, efficiency, and resource utilization.",
	"Networking - RateLimiting - Algorithms - TokenBucket": "Description: Uses tokens to control traffic flow. Tokens are added to a bucket at a regular rate and requests consume tokens. If the bucket runs out of tokens, new requests are denied. Example: A bucket can hold 10 tokens and 1 token is added every 10 seconds. A request needs 1 token to pass. If there's a sudden burst of 15 requests, only 10 can go through, and subsequent requests must wait for new tokens.",
	"Networking - RateLimiting - Algorithms - LeakyBucket": "Description: Requests are added to a queue (bucket) and processed at a fixed rate to smooth out burst traffic. Example: If the bucket size is 10 and the rate is 1 request per second, and a burst of 20 requests comes in, the first 10 are queued and processed at 1 per second, while the rest are either queued (if the bucket can hold them) or discarded.",
	"Networking - RateLimiting - Algorithms - FixedWindowCounter": "Description: Divides time into fixed windows and counts the number of requests in each window. Example: If the limit is 100 requests per hour, and a user makes 100 requests in the first half-hour, they will be blocked for the remaining half-hour, even if the server is underutilized during that time.",
	"Networking - RateLimiting - Algorithms - SlidingWindowCounter": "Description: A hybrid of the fixed window and the sliding log, offering a balance between efficiency and precision. It combines the fixed window's simplicity and the sliding log's accuracy. Example: If the limit is 100 requests per hour, the server counts requests in the current window and a fraction of the requests from the previous window, based on the time elapsed.",
	"Networking - RateLimiting - Algorithms - SlidingWindowLog": "Description: Keeps a time-stamped log of requests. It checks whether adding a new request would exceed the rate limit, considering the time frame. Example: If the limit is 100 requests per hour, each incoming request is checked against the log of requests in the past hour. Older entries are discarded.",
	"Networking - RateLimiting - Applications - APIs": "To control traffic and prevent abuse.",
	"Networking - RateLimiting - Applications - WebServers": "To control traffic and prevent abuse.",
	"Networking - RateLimiting - Applications - NetworkTraffic": "To control data flow in networks.",
	"Networking - RateLimiting - Applications - ApplicationServers": "To prevent overload and ensure fair usage.",
	"Networking - ReverseProxy": "A Reverse Proxy is a type of proxy server that retrieves resources on behalf of a client from one or more servers.",
	"Networking - ReverseProxy - Location": "It sits between the client and the backend services or servers.",
	"Networking - ReverseProxy - Functions": "A Reverse Proxy focuses more on network-level concerns like load balancing, security, and caching for a wider range of applications.",
	"Networking - ReverseProxy - Functions - traffic - LoadBalancing": "Distributes client requests across multiple servers to balance load and ensure reliability.",
	"Networking - ReverseProxy - Functions - security": "Provides an additional layer of defense (hides the identities of backend servers).",
	"Networking - ReverseProxy - Functions - security - SSL": "Handles SSL encryption and decryption, offloading that responsibility from backend servers.",
	"Networking - ReverseProxy - Functions - integration - Caching": "Can cache content to reduce server load and improve performance.",
	"Networking - ForwardProxy": "A Forward Proxy, also known simply as a proxy, acts as an intermediary between client devices and the internet. It facilitates requests from clients to external servers, providing functionalities such as content filtering, access control, and anonymity.",
	"Networking - ForwardProxy - Functions - ContentFiltering": "Organizations use Forward Proxies to control and monitor internet usage within their network. Proxies can filter out malicious content, restrict access to certain websites, and enforce usage policies.",
	"Networking - ForwardProxy - Functions - Anonymity": "Forward Proxies provide a level of anonymity for clients by masking their IP addresses. This is particularly useful for users seeking to access the internet without revealing their identity.",
	"Networking - LoadBalancer": "A Load Balancer is a network distributor at Layer 4 or 7 of OSI Model.",
	"Networking - LoadBalancer - Algorithms": ".",
	"Networking - LoadBalancer - Algorithms - RoundRobin": "It distributes incoming requests to each server in a sequential manner. When a request comes in, it is sent to the first server in the list, the second request goes to the second server, and so on, cycling back to the first server after reaching the last one.",
	"Networking - LoadBalancer - Algorithms - RoundRobin - UseCases": "Web servers (where requests are similar) Any environment where servers are relatively equal in capability and performance.",
	"Networking - LoadBalancer - Algorithms - RoundRobin - Advantages": "Simple to implement and understand. No need for complex monitoring or health checks.",
	"Networking - LoadBalancer - Algorithms - RoundRobin - Disadvantages": "Does not account for server load or capability, which may lead to resource imbalance. If servers have different processing power, it can cause performance issues.",
	"Networking - LoadBalancer - Algorithms - StickyRoundRobin": "Sticky Round Robin is a variation of the Round Robin approach that maintains session persistence, meaning that once a client is connected to a server, subsequent requests from that client are directed to the same server as long as the session is active.",
	"Networking - LoadBalancer - Algorithms - StickyRoundRobin - UseCases": "Applications that maintain user sessions (e.g., e-commerce sites, online banking).",
	"Networking - LoadBalancer - Algorithms - StickyRoundRobin - Advantages": "Improved user experience due to session persistence. Reduces overhead of managing user session data across multiple servers.",
	"Networking - LoadBalancer - Algorithms - StickyRoundRobin - Disadvantages": "Can lead to uneven load distribution if some users generate more traffic than others. If the server fails, the user may lose their session.",
	"Networking - LoadBalancer - Algorithms - WeightedRoundRobin": "Weighted Round Robin assigns a weight to each server based on its capacity or capability. Servers with higher weights receive more requests than those with lower weights. The scheduler distributes requests according to the defined weights.",
	"Networking - LoadBalancer - Algorithms - WeightedRoundRobin - UseCases": "Applications with servers of varying capabilities (e.g., a mix of high-performance and standard servers).",
	"Networking - LoadBalancer - Algorithms - WeightedRoundRobin - Advantages": "Better resource utilization compared to standard round robin. Can handle uneven server capabilities more effectively.",
	"Networking - LoadBalancer - Algorithms - WeightedRoundRobin - Disadvantages": "Requires monitoring and configuration of server weights. More complex to implement than basic Round Robin.",
	"Networking - LoadBalancer - Algorithms - Hash": "The Hash algorithm uses a hash function to determine which server should handle a request based on certain attributes (such as the client's IP address or session ID). This ensures that requests from the same client are consistently directed to the same server.",
	"Networking - LoadBalancer - Algorithms - Hash - UseCases": "Applications requiring consistent routing for user sessions. Scenarios where user data needs to be kept on the same server.",
	"Networking - LoadBalancer - Algorithms - Hash - Advantages": "Ensures consistent routing of requests from the same client. Reduces the overhead of session management.",
	"Networking - LoadBalancer - Algorithms - Hash - Disadvantages": "Can lead to uneven load distribution if certain clients generate more traffic. Changes in server topology (adding/removing servers) can disrupt the load balancing.",
	"Networking - LoadBalancer - Algorithms - LeastConnections": "The Least Connections algorithm directs incoming requests to the server with the fewest active connections. This is particularly useful when servers have varying capabilities or when requests have variable processing times.",
	"Networking - LoadBalancer - Algorithms - LeastConnections - UseCases": "Applications where connections can remain open for long periods (e.g., database connections, file uploads).",
	"Networking - LoadBalancer - Algorithms - LeastConnections - Advantages": "More efficient use of resources, as it directs traffic to less busy servers. Adapts better to changing loads and server performance.",
	"Networking - LoadBalancer - Algorithms - LeastConnections - Disadvantages": "Requires real-time monitoring of active connections. May not be effective in environments with very short-lived connections.",
	"Networking - LoadBalancer - Algorithms - WeightedLeastConnections": "",
	"Networking - LoadBalancer - Algorithms - LeastResponseTime": "The Least Response Time algorithm routes requests to the server that has the lowest response time for previous requests. It requires continuous monitoring of server performance metrics.",
	"Networking - LoadBalancer - Algorithms - LeastResponseTime - UseCases": "Applications where response time is critical (e.g., real-time applications, streaming services).",
	"Networking - LoadBalancer - Algorithms - LeastResponseTime - Advantages": "Ensures optimal performance by directing traffic to the most responsive server. Better user experience in latency-sensitive applications.",
	"Networking - LoadBalancer - Algorithms - LeastResponseTime - Disadvantages": "Requires constant monitoring and can introduce complexity. Fluctuations in response time can lead to erratic routing decisions.",
	"Networking - LoadBalancer - Algorithms - LeastBandwith": "It directs traffic to the server using the least network bandwidth at the moment.",
	"Networking - LoadBalancer - Algorithms - LeastBandwith - Usage": "Useful when managing network usage is important to prevent congestion.",
	"Networking - LoadBalancer - Algorithms - Random": "It picks a server at random for each new request.",
	"Networking - LoadBalancer - Algorithms - Random - Usage": "Useful when you don't need to consider server load or differences in server capacity.",
	"Networking - LoadBalancer - Tools - NGINX": "",
	"Networking - LoadBalancer - Tools - HAProxy": "",
	"Networking - LoadBalancer - Tools - AWS ELB": "",
	"Networking - LoadBalancer - Tools - Ribbon": "",
	"Networking - LoadBalancer - Tools - SpringCloudLoadBalancer": "",

	"Processing": "SECTION",
	"Processing - Servers": "",
	"Processing - Servers - Java - SpringBoot": "",
	"Processing - Servers - Java - Micronaut ": "",
	"Processing - Servers - Java - Quarkus ": "",
	"Processing - Servers - Java - Vert.x: ": "",
	"Processing - Servers - Java - Dropwizard ": "",
	"Processing - Servers - Java - Helidon ": "",
	"Processing - Servers - Java - Lagom ": "",
	"Processing - Servers - Java - KumuluzEE ": "",
	"Processing - Serverless": "Use a deployment infrastructure that hides any concept of servers (i.e. reserved or preallocated resources)- physical or virtual hosts, or containers. The infrastructure takes your service’s code and runs it. You are charged for each request based on the resources consumed. To deploy your service using this approach, you package the code (e.g. as a ZIP file), upload it to the deployment infrastructure and describe the desired performance characteristics. The deployment infrastructure is a utility operated by a public cloud provider. It typically uses either containers or virtual machines to isolate the services. However, these details are hidden from you. Neither you nor anyone else in your organization is responsible for managing any low-level infrastructure such as operating systems, virtual machines, etc.",
	"Processing - Serverless - Tools - Firebase": "Firebase by Google provides a complete suite of backend services, including real-time databases, authentication, hosting, and cloud functions.",
	"Processing - Serverless - Tools - Susabase": "Often referred to as the open-source alternative to Firebase, Supabase offers a powerful Postgres database, authentication, and real-time capabilities.",
	"Processing - Serverless - Tools - Backendless": "Backendless is a no-code/low-code platform that provides backend features like databases, APIs, and user management.",
	"Processing - Serverless - Tools - Netlify": "Netlify combines frontend hosting with backend-like functionalities, such as serverless functions and database integrations.",
	"Processing - Serverless - Tools - Zapier": "Zapier connects different web applications, automating workflows and reducing the need for custom backend APIs.",
	"Processing - Serverless - Tools - NotionAPI": "Notion’s API allows developers to use its robust database and organization tools as a backend.",
	"Processing - Serverless - Tools - AWSAmplify": "AWS Amplify is a powerful toolset for building serverless applications. It provides backend services like authentication, storage, and APIs without needing to code backend logic.",
	"Processing - Serverless - Tools - Bubble": "Bubble is a no-code platform that lets you build full-stack applications without touching backend code.",
	"Processing - Serverless - Tools - Hasura": "Hasura is a GraphQL engine that lets you build scalable APIs over your databases instantly."

}